# Model Debugging & Generalization - Interview Notes

---

## 1. Model Underperforming on Test Data — Debugging Checklist

- Verify **data leakage** (features using future info, target encoded incorrectly).  
- Check **train/validation distributions** (feature drift, label distribution mismatch).  
- Compare **train vs val metrics** → overfitting (big gap) vs underfitting (both bad).  
- Inspect **feature importance**, data quality, and label noise.  
- Revisit **evaluation metric** – is it aligned with the business objective?

---

## 2. Overfitting Remedies

- Add **regularization** (L1/L2, weight decay), **dropout**, or early stopping.  
- Simplify the model: fewer layers/features, smaller trees, lower max depths.  
- Use **data augmentation** (images, text, audio) and gather more data if possible.  
- Use proper **cross-validation** and tune hyperparameters with a validation set.  

---

## 3. Underfitting Remedies

- Increase model capacity: deeper networks, more trees, higher max depth.  
- Reduce regularization strength and train for more epochs/iterations.  
- Add more expressive features or non-linear transformations.  
- Check data preprocessing: no over-aggressive dimensionality reduction or smoothing.

---

## 4. Handling Missing Data

- Simple imputations: **mean/median** for numeric, **most frequent** for categorical.  
- Use **indicator flags** for “was missing” as additional features.  
- Apply more advanced methods: **KNN imputer**, model-based imputation.  
- For tree-based models, sometimes “missing as its own category” works well.

---

## 5. Handling Outliers (3 Methods)

- **Trimming/Winsorizing:** remove or cap extreme values at certain percentiles.  
- **Robust transformations:** log/Box-Cox transforms or robust scalers (median/IQR).  
- **Model-based approaches:** use tree ensembles (less sensitive) or add outlier-resistant loss (e.g., Huber).  

---

## 6. Long-Tailed Distribution & Handling Long-Tail Imbalance

- Long tail = many **rare classes or cases** with few samples.  
- Use **re-sampling** (oversample tail, undersample head) or **class weights**.  
- Aggregate ultra-rare classes if acceptable, or use **hierarchical labels**.  
- For predictions, adjust thresholds per class and evaluate with per-class metrics.

---

## 7. Focal Loss

- Focal loss down-weights **easy, well-classified examples** and focuses on the hard ones.  
- Common in extreme class imbalance (e.g., object detection, rare event prediction).  
- Adds a modulating factor `(1 - p)^gamma` to cross-entropy so confident predictions get less weight.  
- Helps the model pay more attention to **minority or difficult samples**.

---

## 8. Regularization Intuition

- Regularization adds a **penalty on model complexity** (e.g., large weights).  
- Forces the model to choose **simpler explanations** that generalize better.  
- It shrinks parameters towards zero or reduces reliance on any single feature.  
- Intuitively: “prefer smooth, simple functions that still fit the data.”

---

## 9. Why Train on Mini-Batches?

- Mini-batches exploit **vectorized operations on GPUs/TPUs** efficiently.  
- Provide a **noisy but good** estimate of the full gradient, improving generalization.  
- Pure full-batch is slow and memory-heavy; pure SGD (single sample) is too noisy.  
- Mini-batches are the sweet spot between speed, stability, and noise.

---

## 10. Why Validation Loss and Accuracy May Increase Together?

- Accuracy is coarse: it only checks **correct vs incorrect**, not confidence.  
- Loss captures **confidence in predictions**; overconfident wrong predictions increase loss.  
- It’s possible to classify slightly more samples correctly (accuracy up) but with worse-calibrated probabilities (loss up).  
- Always look at **both metrics** together, especially for probabilistic models.


