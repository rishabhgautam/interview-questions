
# Popular Machine Learning Algorithms - Interview Notes

---

## 1. Linear Regression — Pros, Cons, Assumptions

**Pros:**  
✔ Highly interpretable and easy to implement  
✔ Fast to train and computationally efficient  
✔ Works well when relationship is linear  
✔ Good baseline model  

**Cons:**  
✘ Poor performance with non-linear relationships  
✘ Sensitive to outliers  
✘ Struggles with multicollinearity  
✘ Assumes constant variance and independence  

**Key Assumptions:**  
- **Linearity:** Relationship between features and target is linear  
- **Independence:** Residuals (errors) are independent  
- **Homoscedasticity:** Constant variance in residuals across predictions  
- **No multicollinearity:** Features are not strongly correlated  
- **Normality of residuals:** Errors follow a normal distribution  

**Mental Hook:** *“Straight-line thinking with clean, uncorrelated data and calm errors.”*


---

## 2. Logistic Regression — Pros, Cons, Why “Linear”?

**Pros:**  
✔ Produces well-calibrated probabilities  
✔ Interpretable coefficients (log-odds)  
✔ Fast to train, scalable to large datasets  
✔ Works well with regularization (L1/L2)

**Cons:**  
✘ Can’t naturally model complex non-linear relationships  
✘ Performance drops without good feature engineering  
✘ Assumes linear relationship between features and log-odds  
✘ Sensitive to outliers and multicollinearity

**Why called “Linear”?**  
Because the decision boundary is defined by a linear equation:  
\[
w^T x + b = 0
\]  
Logistic regression applies a **sigmoid** activation to this linear combination to convert it into probabilities, but the underlying separation of classes is still based on a **linear hyperplane**.

**Mental Hook:**  
➡ *Linear boundary, nonlinear output.*


---

## 3. Support Vector Machines (SVM) — Pros, Cons, Kernel Trick

**Pros:** Effective in high-dimensional spaces, maximizes margin, robust to overfitting with proper C and kernel.  
**Cons:** Slow on very large datasets, sensitive to choice of kernel and hyperparameters, less interpretable.  
**Kernel trick:** Implicitly maps data into a higher-dimensional feature space using kernels (RBF, polynomial, etc.) without computing the mapping explicitly.  
This allows learning non-linear decision boundaries using a linear separator in the transformed space.

---

## 4. Decision Trees — Pros, Cons, Cost Functions, Splits

**Pros:**  
✔ Highly interpretable and easy to visualize  
✔ Handles both numerical and categorical features  
✔ Naturally captures non-linear decision boundaries  
✔ Minimal data preprocessing (no need for scaling or normalization)  
✔ Can handle missing values (depending on implementation)

**Cons:**  
✘ Highly prone to overfitting (high variance)  
✘ Small changes in data can lead to very different trees (instability)  
✘ Greedy splitting may miss globally optimal solutions  
✘ Not great for extrapolation or smooth predictions

**Cost Functions:**  
- **Classification:**  
  - **Gini impurity:** Measures how often a randomly chosen sample would be incorrectly classified.  
    \[
    G = 1 - \sum p_i^2
    \]  
  - **Entropy (Information Gain):** Measures uncertainty in the data.  
    \[
    H = -\sum p_i \log_2 p_i
    \]

- **Regression:**  
  - **MSE (Mean Squared Error)** or **MAE (Mean Absolute Error)** used to evaluate split quality.

**How Splitting Works:**  
- Greedy search: at each node, evaluates **all features** with possible split points.  
- Chooses the split that yields the **largest impurity reduction** (classification) or **largest variance reduction** (regression).  
- Process repeats recursively until stopping criteria (max depth, min samples) are met.

**Mental Hook:**  
“Decision Trees slice the space into boxes to make decisions — but too many slices lead to overfitting.”


---

## 5. Random Forest — Pros, Cons

**Pros:** Reduces variance via averaging many trees, handles non-linearities, robust to overfitting and noise, works well out of the box.  
**Cons:** Less interpretable than a single tree, can be slow and memory-heavy for many trees and features.  
Good default choice for tabular data, especially when you don’t want heavy feature engineering.

---

## 6. K-Nearest Neighbors (KNN) — Pros, Cons, Distance-Weighted KNN

**Pros:** Very simple, non-parametric, adapts to complex decision boundaries, essentially no training time.  
**Cons:** Prediction is slow for large datasets, sensitive to irrelevant features and feature scaling, memory-intensive.  
**Distance-weighted KNN:** Neighbors closer to the query point get higher weight (for example, 1 / distance), improving robustness.  
Works best on low-dimensional, well-scaled data.

---

## 7. K-means — Pros, Cons, Convergence, Empty Clusters

**Pros:** Simple, fast, scalable, widely used for clustering with roughly spherical clusters.  
**Cons:** Assumes convex, roughly equal-sized clusters; sensitive to initialization and outliers; needs K specified in advance.  
**Convergence:** Guaranteed to converge in finite steps to a local minimum of within-cluster variance (but not the global minimum).  
**Empty clusters:** Can occur when no points are assigned to a centroid; often handled by reinitializing that centroid or merging with another cluster.

---

## 8. PCA — Pros, Cons

**Pros:** Reduces dimensionality, decorrelates features, often improves speed and reduces noise; helpful for visualization and compression.  
**Cons:** Linear method, may miss non-linear structure; components are less interpretable; sensitive to feature scaling.  
Works best when variance directions are meaningful and you are okay losing direct feature semantics.

---

## 9. Naive Bayes — Pros, Cons, Independence Assumption

**Pros:** Extremely fast, low data requirement, works surprisingly well in text classification and other high-dimensional settings.  
**Cons:** Strong independence assumption rarely holds in reality; probability estimates can be poorly calibrated.  
**Independence assumption:** Features are conditionally independent given the class label, which simplifies `P(x|y)` into a product of per-feature terms.  
Despite being “naive”, it is often a very strong baseline.

---

## 10. AdaBoost — Pros, Cons, Process

**Pros:** Focuses on hard-to-classify samples, can turn weak learners (for example, decision stumps) into a strong ensemble, often achieves high accuracy on tabular data.  
**Cons:** Sensitive to noisy labels and outliers, can overfit if too many rounds or overly complex base learners are used.  

**Process (High-level):**  
- Start with uniform weights over all training samples.  
- Train a weak learner and compute its weighted error.  
- Assign a learner weight (alpha) based on its error: better learners get higher alpha.  
- Increase weights of misclassified samples and decrease weights of correctly classified ones so the next learner focuses on “hard” points.  
- Final prediction is a weighted vote of all weak learners (for classification, the sign of the weighted sum).

---

## 11. Gradient Boosting vs Random Forest — Differences

**Random Forest:**  
- Bagging of trees; trees trained independently on bootstrapped samples.  
- Mainly reduces variance by averaging many high-variance trees.  

**Gradient Boosting:**  
- Trees added sequentially; each new tree fits the residuals (errors) of the previous ensemble.  
- Mainly reduces bias by gradually improving the fit.  

Random Forest is more robust and easier to tune; Gradient Boosting often achieves higher accuracy but needs careful regularization and parameter tuning.

---

## 12. GBDT — Pros, Cons, Sparse Data Handling

**Pros:** State-of-the-art on many tabular tasks, handles complex non-linear interactions, supports various loss functions (regression, classification, ranking).  
**Cons:** Sensitive to hyperparameters, can overfit if not regularized, slower to train than single models.  
**Sparse data:** Implementations like XGBoost and LightGBM efficiently handle sparse matrices, missing values, and use optimized split-finding algorithms.  
Excellent choice for structured/tabular data when correctly tuned.

---

## 13. Why GBDTs Are Inefficient for Online Learning

GBDTs build trees sequentially, with each new tree depending on the residuals from all previous trees.  
To incorporate new data, you would need to recompute residuals and potentially rebuild many trees.  
Individual trees are not naturally incremental, unlike linear models or some online algorithms.  
Therefore, GBDTs are usually retrained in batches (offline or mini-batch) rather than updated per-sample.

---

## 14. Ensemble Methods — Types and Why They Help

**Types:** Bagging, boosting, stacking, and voting ensembles.  
They combine multiple models to reduce variance, reduce bias, or leverage different model strengths.  
Bagging reduces variance by averaging diverse high-variance models; boosting reduces bias by sequentially correcting errors; stacking learns an optimal meta-combination of base models.  
Overall, ensembles usually generalize better than individual models.

---

## 15. Bagging, Boosting, XGBoost

**Bagging:**  
- Train models on bootstrapped samples and average their predictions (for example, Random Forest).  
- Mainly reduces variance and improves stability.

**Boosting:**  
- Sequentially add weak learners that focus on previous mistakes (for example, AdaBoost, Gradient Boosting).  
- Mainly reduces bias by iteratively improving the model.

**XGBoost:**  
- Highly optimized Gradient Boosting implementation with regularization, efficient split finding, sparsity-aware algorithms, and strong system-level optimizations.  
- Widely used in competitions and production systems for tabular data.

---

## 16. Autoencoder vs Variational Autoencoder (VAE)

**Autoencoder (AE)**  
- Has an encoder that maps input `x` to a latent vector `z`, and a decoder that reconstructs `x_hat` from `z`.  
- Trained to minimize reconstruction loss between `x` and `x_hat` (for example, MSE or cross-entropy).  
- Learns a deterministic latent representation (same `x` gives same `z`), useful for compression, denoising, and dimensionality reduction.  
- Not inherently probabilistic, so random latent samples may not decode to realistic data.

**Variational Autoencoder (VAE)**  
- Encoder outputs parameters of a distribution over `z` (typically mean and log-variance) instead of a single vector.  
- Uses the reparameterization trick: sample `epsilon` from a standard normal and compute `z = mu + sigma * epsilon` so gradients can flow through sampling.  
- Loss = reconstruction loss plus a KL-divergence term that regularizes the latent distribution towards a simple prior (usually standard normal).  
- Because the latent space is smooth and structured, you can sample `z` from the prior and decode it to generate new, realistic data.

---

## 17. EM Algorithm (Expectation-Maximization)

The EM algorithm is an iterative method to find maximum likelihood estimates when the model involves hidden or latent variables.  
**E-step:** Estimate the expected values of hidden variables given the current parameter estimates.  
**M-step:** Maximize the expected complete-data log-likelihood to update parameters.  
EM alternates E and M steps until convergence and is used in models like Gaussian Mixture Models (GMMs) and Hidden Markov Models (HMMs).

---

## 18. K-means: How to Choose K?

Common methods include:  
- **Elbow method:** Plot SSE (sum of squared errors) vs K and look for a “knee” where additional clusters give diminishing returns.  
- **Silhouette score:** Choose K that maximizes the average silhouette score, which measures how well points fit within their clusters vs others.  
- Information criteria (AIC/BIC) can guide K under certain modeling assumptions.  
In practice, combine these with domain knowledge, interpretability, and downstream task performance to pick K.

```
