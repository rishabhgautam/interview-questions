# Neural Network Architectures - Interview Notes

---

## 1. ANN — Pros / Cons

**Artificial Neural Networks (ANNs)**: classic multi-layer perceptrons (MLPs) mapping fixed-size inputs → outputs.  

**Pros:**  
- Universal function approximators: can model **any continuous function** with enough neurons/layers.  
- Flexible and simple to implement.  
- Effective for tabular and low-dimensional structured data.  
- Compatible with standard optimization algorithms (SGD, Adam).  

**Cons:**  
- Do not exploit structure in data (spatial/temporal) → inefficient for images or sequences.  
- Prone to **overfitting** if network is too large.  
- Require careful tuning: number of layers, neurons, learning rate, initialization, regularization.  
- Black-box: limited interpretability compared to simpler ML models.

---

## 2. CNN — Structure, Layers, Activation, Kernel Size

**Convolutional Neural Networks (CNNs)**: exploit **local spatial structure**.  

**Typical Layer Stack:**  
`Conv → Activation (ReLU/LeakyReLU) → Pooling → (repeat) → Fully Connected → Output`

**Layer Details:**  
- **Convolution:** filters slide over input; small kernel (3×3, 5×5) captures local features.  
- **Activation:** non-linearity, typically ReLU, LeakyReLU; mitigates vanishing gradients.  
- **Pooling:** max/average downsample spatial resolution → reduces parameters and improves translation invariance.  
- **BatchNorm:** stabilizes and accelerates training.  
- **Fully Connected:** integrates high-level features for final output.  

**Practical Tips:**  
- Small kernels (3×3) stacked → large receptive field.  
- Use strides/pooling for downsampling instead of large kernels → fewer parameters.  
- Typical CNNs: LeNet, AlexNet, VGG, ResNet, EfficientNet.

---

## 3. CNN vs FCNN vs FFNN — Differences

| Aspect | FFNN / FCNN | CNN |
|--------|------------|-----|
| Input | Fixed-size vector | Grid/structured input (image, video) |
| Connectivity | Fully connected | Local receptive fields + weight sharing |
| Parameters | Many | Fewer due to shared weights |
| Translation invariance | No | Yes (convolution + pooling) |
| Best Use | Tabular data | Images, video, spatial data |

---

## 4. RNN Motivation

**Recurrent Neural Networks (RNNs)** handle **sequential data**:  
- Maintain **hidden state** `h_t` that carries info from previous time steps.  
- Suitable for **text, speech, time series**.  
- Can, in principle, model **arbitrary-length context** (limited by vanishing gradients).  

**Limitations:**  
- Vanilla RNNs struggle with **long-term dependencies** due to vanishing/exploding gradients.  
- Sequential computation → slower training; cannot fully leverage GPU parallelism.

---

## 5. LSTM vs Vanilla RNN — Problem Solved

**LSTM (Long Short-Term Memory):**  
- Introduces **gates**: input, forget, output → control flow of information.  
- **Cell state** `C_t` preserves long-term memory.  
- Gradients can flow unchanged → mitigates vanishing gradient problem.  

**GRU (Gated Recurrent Unit):**  
- Simpler alternative: combines input & forget gates into a single update gate.  
- Often performs similarly to LSTM with fewer parameters.  

**Use Cases:**  
- Text generation, speech recognition, stock prediction, sequential recommendation systems.

---

## 6. Memory Networks

- Neural networks augmented with **explicit memory modules**.  
- Components:  
  - Memory write: store information  
  - Memory read: attention or retrieval from memory  
  - Output: reasoning based on memory contents  
- **Applications:** question answering, reasoning over long context, dialogue systems.  
- Inspired architectures: **Neural Turing Machines (NTM), Differentiable Neural Computers (DNC)**, RAG models.

---

## 7. Capsule Networks (CapsNets)

- Group neurons into **capsules** encoding:  
  - Probability of feature presence  
  - Pose/orientation of features  
- Use **dynamic routing** instead of pooling → preserves spatial hierarchies.  
- Goals:  
  - Recognize objects under rotation or part rearrangement.  
- Challenges:  
  - Computationally expensive  
  - Hard to scale to large datasets  
- Practical adoption: limited; research ongoing.

---

## 8. Autoencoders (AE)

- Learn **compact latent representations** by reconstructing inputs.  
- Structure: `Encoder → Latent Space → Decoder`  
- **Loss:** reconstruction error (MSE, BCE).  
- Uses:  
  - Dimensionality reduction  
  - Denoising  
  - Anomaly detection  
  - Pretraining for downstream tasks

---

## 9. Variational Autoencoders (VAE)

- Probabilistic version of AE → learns **latent distribution** instead of single point.  
- Encoder outputs **mean μ and variance σ²** → latent vector sampled via **reparameterization trick**.  
- Loss = `Reconstruction Loss + KL Divergence` (regularizes latent space).  
- **Generative:** sample from prior N(0,1) → decode → new realistic data.

---

## 10. Parameter Sharing in DL

- **Concept:** reuse same weights across different positions/timesteps.  
- Examples:  
  - CNN: same kernel applied across image  
  - RNN: same weight matrices at each time step  
- Advantages:  
  - Reduces parameters → less memory  
  - Encodes inductive biases (translation/time invariance)  
  - Improves generalization

---

## 11. Self-Supervised Learning — Examples

- Labels derived from **data itself** → no human annotation needed.  
- NLP:  
  - Masked Language Modeling (BERT)  
  - Next Token Prediction (GPT)  
  - Sentence Order Prediction  
- Vision:  
  - Predict rotation angle  
  - Contrastive learning (SimCLR, MoCo)  
  - Masked Autoencoders (MAE)  
- Benefits: pretrain on large data → fine-tune on small labeled dataset.

---

## 12. Curriculum Training

- **Idea:** train model on **easy examples first**, then gradually harder.  
- Benefits:  
  - Stabilizes training  
  - Faster convergence  
  - Sometimes improves generalization  
- Variants:  
  - Self-paced learning: model decides which samples to learn first  
  - Task curriculum: easy subtasks → complex tasks

---

## 13. Advanced Architectures / Trends

- **Transformers:** self-attention → parallel sequence modeling; replaces RNNs in NLP & vision (ViT).  
- **Graph Neural Networks (GNNs):** handle relational data → social networks, molecules.  
- **Diffusion Models:** generative → image/audio generation, denoising.  
- **Attention + Memory hybrids:** RAG, Perceiver, Memory-augmented Transformers.

---

## 14. Practical Tips

- Pretrain whenever possible (transfer learning).  
- Normalize inputs → stabilize gradients.  
- Monitor **train/val loss and metrics** → early stopping.  
- Inspect activation distributions → avoid dead neurons (ReLU) or saturation (sigmoid/tanh).  
- Choose architecture based on **data type, task, compute constraints**.
