# ğŸš€ ML System Design Interview Handbook

---

## 1. ML System Design Framework

| Stage | What You Must Clarify | Interview Signals |
|------|----------------------|------------------|
| **Business Framing** | Goal, KPIs, constraints | Knows problem > model |
| **Data Strategy** | Sources, freshness, ground truth | 90% time is data |
| **Feature Strategy** | Offline features vs streaming | Minimizes leakage |
| **Modeling Strategy** | Model class, latency needs | Applied modeling depth |
| **Training Infra** | Distributed? GPUs? MLOps maturity | Practical scalability |
| **Serving Strategy** | Batch vs real-time vs edge | User experience impact |
| **Monitoring** | Drift, decay, fairness, alerts | Post-deployment thinking |
| **Governance & Cost** | Privacy, PII, compliance, $ | Responsible ML |

> â— Interview tip: Say **â€œOnly add complexity if business requires it.â€**

---

## 2. ML System Architecture (High-Level Blueprint)

```

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
```

Logs     â”‚              â”‚
Events â†’  â”‚  Data Lake   â”‚  â† CRM, APIs
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ ETL/Batch + Stream
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Feature Store      â”‚
â”‚  (Offline + Online)   â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
Offline       Realtime Features
Training      (low-latency)
â”‚              â”‚
â–¼              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚ Model Training â”‚ â—„â”€â”€â”€â”€â”˜ feedback loop
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ artifacts (registry + versioning)
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Serving (API/Batch)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â–¼
Monitoring & Drift

```

---

## 3. Feature Engineering + Feature Store

| Feature Group | Examples | Notes |
|---------------|----------|------|
| **Static** | Age, signup method | Slow-changing |
| **Dynamic (aggregates)** | 7-day spend, churn score | Needs versioning |
| **Real-time** | Time since last click | Requires streaming |
| **Cross features** | Product Ã— Region embeddings | Improves personalization |

**Feature Store Capabilities**
- Point-in-time correctness (no leakage)
- Reuse across teams (avoid duplication)
- Online/offline **consistency**
- Automated validation & quality checks
- Lineage + governance

ğŸ§  If it affects prediction **after** inference â†’ feature drift

---

## 4. Model Strategy & Training

### âœ¨ Model Choice Based on Use Case
| Use Case | Realistic Models |
|---------|------------------|
| Fraud / Risk | GBDT (XGBoost/LightGBM), GANs for anomalies |
| Reco Systems | Two-tower retrieval + DL ranking + re-ranking |
| NLP | BERT/LLM + RAG + fine-tuning (LoRA, QLoRA) |
| Tabular BI | Trees > DNNs (unless cross features add value) |
| Forecasting | TCN, TFT, Prophet + exogenous signals |

### ğŸ‹ï¸ Training Paradigms
- Offline batch (common in enterprise)
- Continual learning (stream updates)
- Active learning (label budget optimized)
- Transfer + PEFT for LLMs (LoRA, adapters)

ğŸ›  Experiment Tracking: MLflow, WandB, Neptune  
ğŸ“š Model Registry: versioning + staging/prod lifecycle

---

## 5. Model Serving Patterns

| Pattern | Example | Infra |
|--------|---------|------|
| **Batch Scoring** | Nightly churn | Airflow, Spark, Dataflow |
| **Online API** | Fraud scoring < 50ms | FastAPI, Triton, TorchServe |
| **Edge / Mobile** | On-device voice, AR | Quantization, distillation |
| **Hybrid** | Ad ranking (retrieve offline, score online) | Memory store + GPU |

### âš¡ Latency Tiers (Rule of Thumb)
- < **10ms**: In-memory models, CPU optimized
- < **50ms**: GPU or compiled models (TensorRT, Triton)
- < **200ms**: Real-time personalization acceptable
- > **1 min**: Batch acceptable

---

## 6. Monitoring, Drift & Reliability

| Type | Examples | Detection |
|------|----------|-----------|
| **Data Drift** | Shift in feature distributions | Pop tests, KL divergence |
| **Concept Drift** | Target behavior changes | Online performance decay |
| **Latency/Cost** | GPU overload, timeouts | APM + autoscaling metrics |
| **Bias / Fairness** | Protected groups | Group metrics, fairness dashboards |

ğŸ”„ Retraining Triggers:
- Drift > threshold
- New product launch / campaign
- Window-based update (weekly, monthly)
- Business KPI degradation

ğŸ“Œ Tools: Evidently, Arize, Fiddler, Prometheus, Grafana, Sentry

---

## 7. System Design Trade-offs

| Trade-off | Option A | Option B | Question to Ask |
|----------|----------|----------|----------------|
| Latency vs Accuracy | Light model | Large DL | SLA threshold? |
| Cost vs Quality | CPU | GPU | ROI justified? |
| Freshness vs Compute | Batch | Streaming | How stale can data be? |
| Interpretability vs Power | Linear/Tree | Deep nets | Regulated domain? |
| Privacy vs Utility | Local inference | Cloud | PII rules? |

> ğŸ¯ Your job: **Align model choices to business impact**, not accuracy alone.

---

## 8. ML System Design Questions â€” Detailed Sample Answers

| Interview Problem | Key Points |
|------------------|------------|
| **Design Real-Time Fraud Detection** | Streaming (Kafka), low-latency inference, rule + ML hybrid, feature store with time-decay signals, auto-block thresholds |
| **Recommendation System** | Retrieval (ANN/Vector DB) â†’ Ranking â†’ Re-ranking with business rules; embeddings updated nightly |
| **Feature Store** | Registry + offline/online consistency + backfill + lineage; AWS SageMaker/Feast |
| **Concept Drift Handling** | Real-time monitoring, auto-retrain, shadow models, A/B transition |
| **LLM On-Device** | 4-bit quantization, distillation, KV cache, CPU/NPU co-processing |
| **Model Training at Scale** | Distributed training (Horovod/Ray), checkpointing, sharded dataloaders |
| **Continuous Deployment** | Canary release â†’ shadow â†’ traffic shifting |

---

## 9. Privacy, Governance & Security

- PII anonymization, tokenization, data contracts
- GDPR/CCPA compliance, RBAC for sensitive data
- Differential privacy where needed (health/finance)
- Secure model endpoints â†’ Auth, rate limiting, WAF
- Prompt injection & adversarial defenses for LLM systems

---

## 10. Design Template â€” Use in Every Answer

```

1ï¸âƒ£ Business Goal â†’ KPI, constraints
2ï¸âƒ£ Data & Features â†’ batch/stream, PII rules
3ï¸âƒ£ Modeling Approach â†’ latency, accuracy tradeoffs
4ï¸âƒ£ Architecture â†’ training & inference paths
5ï¸âƒ£ MLOps â†’ registry, CI/CD, versioning
6ï¸âƒ£ Monitoring â†’ drift, ROI, alerting
7ï¸âƒ£ Governance â†’ security, privacy compliance

```

> Write this as the first thing â€” **structured thinking wins interviews**.

---

## ğŸ¤ Rapid-Fire Interview Prompts to Practice

- How would you design **churn prediction** for a telco?
- Build an **LLM-based customer support assistant** with RAG.
- Design a **click-through rate** prediction system for ads.
- How to optimize ML cost in AWS?
- Real-time anomaly detection in streaming logs â€” architecture?
- How do you prevent feature leakage?
