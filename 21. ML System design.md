# ğŸš€ ML System Design - Interview Handbook

A complete, structured, mind-map-friendly guide for **Machine Learning System Design interviews**, covering business framing, data flow, architecture, deployment, monitoring, scalability, and real-world FAQs.

---

## ğŸ§­ 1. ML System Design Framework

| Step | Key Questions |
|------|---------------|
| Problem Framing | What business metric? Is ML needed? Latency? Scale? |
| Data Strategy | Where does data come from? Batch vs streaming? Ground truth? |
| Feature Pipeline | ETL, feature store, real-time vs offline features |
| Model Strategy | Type of model? Training paradigm? Offline vs online learning |
| Training Infra | GPUs? Distributed training? Experiment tracking? |
| Serving Strategy | Batch, real-time, edge? Containerization? |
| Evaluation | Offline metrics vs online (A/B, lift, ROI) |
| Monitoring & Maintenance | Drift, decay, logging, retraining triggers |

---

## âš™ï¸ 2. Key ML System Components

### ğŸ—ï¸ Architecture Layers
```

Data Sources â†’ ETL/Streaming â†’ Feature Store â†’ Model Training â†’ Model Registry
â†“                            â†‘
Realtime Serving API â† Model Inference â† Monitoring & Feedback

```

### ğŸŒ Data Sources
- Logs, transactions, IoT, events, APIs, CRM, clickstream
- Batch (S3, BigQuery), Stream (Kafka, Kinesis)

---

## ğŸ“¦ 3. Feature Engineering & Feature Store

| Feature Type | Example |
|--------------|---------|
| Static | Age, gender, signup method |
| Dynamic | Last login, cart value, session time |
| Aggregated | 7-day average spend, click count |
| Real-time Features | Time since last transaction, session device |

ğŸ”¹ Feature Store supports:
âœ“ Versioningâ€ƒâœ“ Offline & Online storeâ€ƒâœ“ Reusabilityâ€ƒâœ“ Consistency

---

## ğŸ¤– 4. Model Selection & Training Strategy

| Use Case | Typical Models |
|-----------|---------------|
| Fraud | Gradient Boosting, CatBoost, DNN |
| Recommendation | Matrix Factorization, Two-Tower, Transformers |
| Forecasting | ARIMA, Prophet, TCN, LSTM |
| NLP | BERT, GPT, T5, LLMs |

**Training Paradigms**  
- Offline batch training (classic ML)  
- Online learning (streaming, incremental)  
- Transfer learning, fine-tuning, LoRA, parameter-efficient training  

**Optimization Tools**: Optuna, Ray Tune, Weights&Biases, MLflow, KubeFlow

---

## ğŸš€ 5. Model Serving Patterns

| Pattern | Use Case |
|---------|----------|
| Batch Scoring | Weekly churn scoring, credit risk |
| Real-Time API | Chatbot, fraud detection, recommendations |
| Edge Inference | Drones, mobile apps, IoT devices |
| Hybrid (Feature Precompute + Realtime) | Ads, personalization |

ğŸ›  Serving Infra: SageMaker, Vertex AI, Docker, TorchServe, Triton, FastAPI

---

## ğŸ›°ï¸ 6. Monitoring & Maintenance

| Type | What to Monitor |
|------|------------------|
| Data Drift | Feature distribution changes |
| Concept Drift | Target behavior changes |
| Latency & Cost | Model response, GPU $, API failures |
| Performance | Accuracy, AUC, business KPIs (lift, ROI) |

ğŸ›  Tools: Evidently AI, Arize AI, Fiddler, Prometheus, Grafana

ğŸ“Œ Retraining Triggers:
- Periodic (monthly/weekly)
- Performance drop
- Drift alert
- New data availability

---

## âš ï¸ 7. System Design Tradeoffs

| Tradeoff | Options |
|----------|---------|
| Latency | Light models vs complex (XGBoost vs BERT) |
| Accuracy | Deep learning vs Explainability |
| Cost vs Scale | GPUs vs CPU clusters |
| Freshness | Real-time vs precomputed |
| Security | Privacy, PII, consent |

---

## ğŸ§ª 8. Frequently Asked ML System Design Questions (High-Level Answers)

| Question | Core Idea |
|----------|-----------|
| Real-time fraud detection | Streaming (Kafka), real-time features, low-latency API, anomaly detection |
| Recommendation system | Collaborative + content-based, two-tower models, feature store, event pipelines |
| Handling model drift | Concept drift detection, A/B testing, scheduled retraining |
| Feature store design | Online/offline consistency, reuse, lineage, real-time updates |
| Personalized ad ranking | Ranking model (LTR), real-time features, retrieval + ranking |
| LLM on-device inference | Quantization (8Bit/4Bit), distillation, LoRA, GPU-lite deployment |
| How to scale ML training | Distributed training (Horovod, Ray), sharding, checkpointing |
| How to deploy at scale | Canary, A/B testing, load balancing, auto-scaling |

---

## ğŸ§  Key Mind Map â€” ML System Design Thinking Template

```

Business Goal
â””â”€â”€ Metrics (KPI, ROI)
â””â”€â”€ Data Strategy (Sources, Frequency)
â””â”€â”€ Feature Engineering
â””â”€â”€ Model Choice & Training
â””â”€â”€ Deployment (Batch / Realtime / Edge)
â””â”€â”€ Monitoring (Drift / Logs / KPIs)
â””â”€â”€ Feedback Loop & Retraining

```

---

## ğŸ“Œ Must-Mention Interview Buzzwords

- Feature drift / concept drift  
- Cold start problem  
- Feature pipeline orchestration  
- Real-time vs offline features  
- Choose model by latency â†’ accuracy tradeoff  
- Embedding, two-stage recommender, vector DB (Pinecone, Faiss)

---

## ğŸ”— Suggested Repo Structure

```

/ml-system-design
â”œâ”€â”€ 1_problem_framing.md
â”œâ”€â”€ 2_data_pipeline.md
â”œâ”€â”€ 3_feature_store.md
â”œâ”€â”€ 4_model_training.md
â”œâ”€â”€ 5_model_serving.md
â”œâ”€â”€ 6_monitoring_and_retraining.md
â”œâ”€â”€ 7_qa_templates.md
â”œâ”€â”€ README.md

```

---

ğŸ”¹ *Designed for ML engineers, data scientists, AI architects targeting mid-to-principal level interviews.*  
```
