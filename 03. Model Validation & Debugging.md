# Model Debugging & Generalization - Interview Notes

---

## 1. Model Underperforming on Test Data - Debugging Checklist

- **Data Leakage:** Ensure no future information or target-related features are inadvertently used. Example: predicting churn using next month's revenue.  
- **Distribution Shifts:** Check if feature distributions differ between train and test (covariate shift) or labels are inconsistent (label shift).  
- **Overfitting vs Underfitting:**  
  - **Overfitting:** Train metric high, test metric low → model too complex, not generalizing.  
  - **Underfitting:** Both train and test metrics low → model too simple, missing patterns.  
- **Feature Importance & Quality:** Evaluate importance scores; check for noisy, missing, or irrelevant features.  
- **Metric Alignment:** Make sure evaluation metric matches business goal (e.g., F1 for imbalanced classes, RMSE vs MAE for regression).  
- **Debugging Tip:** Visualize predictions vs true values, residual plots, or confusion matrices for insights.

---

## 2. Overfitting Remedies

- **Regularization:** L1/L2 penalties, weight decay in NNs.  
- **Dropout / Early Stopping:** Randomly drop neurons during training; stop training once validation loss plateaus.  
- **Model Simplification:** Reduce layers, nodes, features, tree depth.  
- **Data Augmentation:** Rotate, crop, jitter images; synonym replacement in text; pitch shift in audio.  
- **Cross-Validation & Hyperparameter Tuning:** K-fold CV ensures hyperparameters generalize.  
- **Ensemble Methods:** Bagging (Random Forests) reduces variance; stacking can stabilize predictions.

---

## 3. Underfitting Remedies

- **Increase Model Capacity:** Add layers/nodes, increase max depth, more expressive trees.  
- **Reduce Regularization:** L2/L1 penalties may be too strong; relax constraints.  
- **Feature Engineering:** Polynomial features, interactions, embeddings, or non-linear transformations.  
- **Longer Training:** More epochs/iterations if convergence hasn’t happened.  
- **Check Data Preprocessing:** Avoid excessive PCA, scaling mistakes, or dropping informative features.

---

## 4. Handling Missing Data

- **Simple Imputation:** Mean/median for numeric, mode for categorical.  
- **Missing Indicator:** Add a binary flag to denote missingness; helps models detect patterns of absence.  
- **Advanced Imputation:** KNN imputer, multivariate imputation, iterative models.  
- **Tree-based Handling:** Many tree algorithms can naturally handle missing values or treat them as a separate category.

---

## 5. Handling Outliers (3 Methods)

1. **Trimming/Winsorizing:** Cap extreme values (e.g., top 1% and bottom 1%).  
2. **Robust Transformations:** Log, square-root, Box-Cox transformations; use robust scalers (median, IQR).  
3. **Model-based Approaches:** Use models less sensitive to outliers (Random Forests, Gradient Boosted Trees) or loss functions like Huber or quantile loss.  

**Tip:** Visualize with boxplots or z-scores; sometimes outliers carry important signals.

---

## 6. Long-Tailed Distribution & Handling Long-Tail Imbalance

- **Definition:** Many rare classes with few samples; head classes dominate training.  
- **Solutions:**  
  - **Re-sampling:** Oversample tail, undersample head.  
  - **Class Weights:** Penalize mistakes on rare classes more.  
  - **Hierarchical Labels:** Combine ultra-rare classes or create parent categories.  
  - **Threshold Adjustment:** Tune probability thresholds per class.  
- **Evaluation:** Use per-class metrics, F1-score, or macro-averaged metrics rather than overall accuracy.

---

## 7. Focal Loss

- **Motivation:** Standard cross-entropy treats all samples equally; in imbalanced data, easy negatives dominate.  
- **Mechanism:**  
  \[
  FL(p_t) = -\alpha (1-p_t)^\gamma \log(p_t)
  \]  
  - `(1 - p_t)^γ` down-weights easy examples.  
  - `α` balances positive/negative classes.  
- **Use Cases:** Object detection (YOLO), rare event prediction, highly skewed datasets.  
- **Effect:** Focuses learning on hard, misclassified samples, improving minority class performance.

---

## 8. Regularization Intuition

- **Purpose:** Penalize large weights to reduce overfitting.  
- **L1:** Sparse solutions → some weights zero → feature selection.  
- **L2:** Smooth shrinkage → reduces reliance on any single feature.  
- **Bayesian View:** Regularization ≈ prior belief in simpler models.  
- **Intuition:** “Don’t memorize noise; focus on robust, general patterns.”

---

## 9. Why Train on Mini-Batches?

- **Vectorization:** Enables efficient GPU/TPU computation.  
- **Gradient Noise:** Introduces stochasticity → escapes shallow minima, improves generalization.  
- **Memory Efficiency:** Full-batch may not fit in memory; pure SGD is noisy.  
- **Best Practice:** Typical batch sizes: 32, 64, 128; depends on GPU memory & dataset size.

---

## 10. Why Validation Loss and Accuracy May Increase Together?

- **Accuracy:** Binary check (correct/incorrect), ignores prediction confidence.  
- **Loss:** Measures confidence; overconfident wrong predictions ↑ loss.  
- **Scenario:** Slightly more predictions correct (accuracy ↑) but with less calibrated probabilities (loss ↑).  
- **Takeaway:** Always monitor **both metrics**; consider calibration metrics (Brier score, reliability diagrams).

---

## 11. Additional Model Debugging Tips

- **Residual Analysis:** Regression → plot residuals vs predictions/features.  
- **Confusion Matrix / ROC Curves:** Classification → detect bias toward certain classes.  
- **Feature Correlations:** Highly correlated features can cause instability.  
- **Learning Curves:** Plot train vs validation loss over epochs to detect under/overfitting.  
- **Hyperparameter Sensitivity:** Grid or random search to see which params affect generalization most.  
- **Data Visualization:** Outliers, missing values, distribution mismatch often visible via histograms, PCA, t-SNE.

---

## 12. Generalization Best Practices

- Use **regularization** and **early stopping**.  
- Keep **validation/test sets untouched** until final evaluation.  
- Train on **diverse, representative data**; track drift over time.  
- Use **ensembles** for stability (bagging, boosting, stacking).  
- Monitor **confidence calibration**, not just accuracy.  
- Continuously **retrain on fresh data** if distribution changes over time.

---

**Summary Mental Models:**  
- **Overfitting:** Too much memory → simplify + regularize.  
- **Underfitting:** Too little memory → increase capacity + add features.  
- **Imbalance / Long-tail:** Adjust weights / resample / focal loss.  
- **Mini-batches:** Sweet spot for speed, noise, generalization.  
- **Validation mismatch:** Loss ≠ Accuracy → check calibration & confidence.
