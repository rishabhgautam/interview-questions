# LLM Training Paradigms - Interview Notes

---

## 1. Self-Supervised Learning

Self-supervised learning trains models using **labels generated from the data itself**, not human annotations.  
Examples: next-token prediction (GPT), masked language modeling (BERT), next-sentence prediction, contrastive learning.  
The model learns rich representations by solving these “pretext” tasks at scale.  
These representations are then fine-tuned on smaller, labeled downstream tasks.

---

## 2. RLHF – What Is Alignment?

RLHF = Reinforcement Learning from Human Feedback.  
Process: pretrain LLM → collect human preference data over model outputs → train a reward model → use RL (e.g., PPO) to steer the LLM.  
**Alignment** means making the model’s behavior closer to human values and instructions (helpful, honest, harmless), not just maximizing likelihood.  
It’s about pushing the model toward **desired behaviors** under human-defined rewards.

---

## 3. DPO & Bradley–Terry

DPO (Direct Preference Optimization) is a method to align LLMs **directly from preference data**, without an explicit RL loop.  
It uses pairs of model outputs (preferred vs rejected) and optimizes a contrastive objective to favor preferred responses.  
The underlying idea is related to the **Bradley–Terry model**, which models the probability that one item is preferred over another based on their scores.  
DPO simplifies RLHF by turning alignment into a more stable supervised-style training problem.

---

## 4. AE vs VAE vs Diffusion (High-Level)

- **Autoencoder (AE):** Learns a deterministic latent code and reconstructs input; good for compression/denoising, not primarily generative.  
- **Variational Autoencoder (VAE):** Learns a distribution over latent codes with a KL-regularized latent space; can sample from the prior and decode → generative.  
- **Diffusion models:** Start from pure noise and iteratively denoise with a learned model; excel at high-quality image (and now text/audio) generation.  
VAEs give smooth latent spaces; diffusion models give state-of-the-art fidelity and diversity.

---

## 5. Cross-Entropy Non-Convexity in Deep Nets

Cross-entropy loss is **convex** in simple models like logistic regression, but in deep networks the overall loss becomes **non-convex**.  
This is because the network’s mapping from parameters to logits is highly non-linear and compositional.  
Despite many local minima and saddle points, SGD-like methods still find good solutions in practice.  
Modern deep learning relies on this empirical observation: non-convexity is not fatal if the landscape has many “good enough” minima.


