# LLM Training Paradigms - Interview Notes

---

## 1. Self-Supervised Learning (SSL)
- **Definition:** Train models using **labels derived from the input data itself**, no human annotation needed.  
- **Examples in NLP / LLMs:**  
  - **Next-token prediction:** GPT-style causal language modeling  
  - **Masked language modeling:** BERT-style, predict masked tokens  
  - **Next sentence prediction / sentence ordering**  
  - **Contrastive learning:** align similar representations, separate dissimilar ones  
- **Why it works:** Pretext tasks force the model to **learn rich contextual representations**.  
- **Downstream use:** Fine-tune pretrained models on smaller labeled datasets for classification, QA, summarization, etc.  
- **Benefits:**  
  - Scalable to massive datasets  
  - Captures syntax, semantics, and long-range dependencies  
  - Reduces dependency on expensive labeled data  

---

## 2. RLHF – Reinforcement Learning from Human Feedback
- **Goal:** Align LLM behavior with **human preferences and values**.  
- **Pipeline:**
  1. Pretrain a base LLM (self-supervised learning)  
  2. Collect **human preference data** (rank outputs or label “good vs bad”)  
  3. Train a **reward model** to score outputs according to human preferences  
  4. Use **RL (e.g., PPO)** to optimize LLM outputs based on reward  
- **Alignment:** Not just high likelihood; model becomes **helpful, honest, and harmless (HHH)**.  
- **Key challenge:** Balancing model fluency, factuality, and alignment to avoid over-optimization or reward hacking.  

---

## 3. DPO (Direct Preference Optimization) & Bradley–Terry
- **Direct Preference Optimization:**  
  - Aligns LLMs **directly from preference pairs** without an explicit RL loop.  
  - Optimizes a **contrastive objective**: preferred output scores higher than rejected.  
- **Connection to Bradley–Terry model:**  
  - Probability of preference is modeled as a logistic function of score differences:  
    \[
    P(A \succ B) = \frac{\exp(s_A)}{\exp(s_A) + \exp(s_B)}
    \]  
- **Benefits of DPO over RLHF:**  
  - More stable, simpler training  
  - Avoids RL complexities like policy divergence or reward scaling  
  - Still leverages human preference data for alignment  

---

## 4. AE vs VAE vs Diffusion Models
| Model | Latent Representation | Generative? | Key Idea | Use Cases |
|-------|--------------------|-------------|----------|-----------|
| **AE (Autoencoder)** | Deterministic | Limited | Encode → bottleneck → decode | Compression, denoising |
| **VAE (Variational Autoencoder)** | Probabilistic | Yes | KL-regularized latent distribution; sample → decode | Generative modeling, representation learning |
| **Diffusion Models** | Iterative denoising | Yes | Start from noise → iteratively denoise with learned model | High-quality image/audio/text generation, SOTA generative tasks |

- **Comparison:**  
  - **VAE:** smooth latent space, good for sampling and interpolation  
  - **Diffusion:** slower inference, but produces higher fidelity and diversity outputs  
  - **AE:** simpler, not ideal for generation, focuses on reconstruction  

---

## 5. Cross-Entropy Loss & Non-Convexity
- **Cross-entropy:**  
  - Measures difference between predicted probability distribution and target (usually one-hot).  
  - Convex for simple models (e.g., logistic regression).  
- **Deep networks:**  
  - Mapping from parameters → logits is **non-linear and compositional**, making loss **non-convex**.  
  - Non-convexity leads to:
    - Multiple local minima  
    - Saddle points  
    - Plateaus  
- **Optimization reality:**  
  - SGD / Adam can still find “good enough” minima  
  - Empirically, deep nets converge reliably despite non-convexity  
  - Modern networks exploit this property to scale successfully  

---

## 6. Key Takeaways for Interviews
- Understand **why SSL works** and how it differs from supervised learning.  
- Explain **RLHF and alignment** clearly, emphasizing reward modeling and PPO.  
- Compare **DPO vs RLHF**, and link to Bradley–Terry intuition.  
- Distinguish **AE, VAE, and diffusion models** for generative tasks.  
- Explain why **deep nets are non-convex** but still trainable with SGD/Adam.  
- Be ready to discuss trade-offs, scaling, and alignment issues in large LLMs.  
