# Deep Learning (DL) - Fundamentals - Interview Notes

---

## 1. What is Deep Learning?

- Subset of ML using **multi-layer neural networks** to learn hierarchical representations.  
- Learns features **directly from raw data** (images, text, audio) rather than hand-crafted features.  
- Scales well with **large datasets** and **compute accelerators** (GPUs, TPUs).  
- Powers modern NLP (BERT, GPT), vision (ResNet, ViT), and speech systems (Wav2Vec, DeepSpeech).  
- Key benefit: ability to model **complex, non-linear relationships** automatically.

---

## 2. Difference vs Traditional ML

| Aspect | Traditional ML | Deep Learning |
|--------|----------------|---------------|
| Features | Hand-crafted | Learned automatically |
| Model | Simple (LR, SVM, RF) | Deep neural networks |
| Data | Small/medium tabular | Large unstructured |
| Compute | Low | High (GPU/TPU) |
| Interpretability | Often higher | Often lower, needs explainability methods |

---

## 3. Backpropagation

- Computes **gradients of the loss w.r.t parameters** using the **chain rule**.  
- Forward pass computes activations; backward pass computes gradients layer by layer.  
- Optimizers (SGD, Adam, RMSProp) update weights using these gradients.  
- Essential for training deep networks efficiently.

---

## 4. Why Mini-Batch Training?

- Combines **full-batch gradient descent** and **stochastic gradient descent** advantages.  
- Improves **GPU utilization** and memory efficiency.  
- Introduces **noise** that can improve generalization.  
- Typical batch sizes: 32, 64, 128, depending on memory and dataset.

---

## 5. Representation Learning

- Learns **abstract, high-level features** from raw data.  
- Examples:  
  - Images: edges → textures → shapes → objects  
  - Text: words → phrases → sentence meaning → context  
- Pretrained models (BERT, ResNet) provide **transferable representations**.  
- Makes downstream tasks easier: classification, clustering, generation.

---

## 6. Dropout

- Randomly sets a fraction of neurons to 0 during training.  
- Prevents **co-adaptation** and reduces overfitting.  
- Scaling applied during inference: activations multiplied by keep probability.  
- Typical keep probability: 0.5 for fully connected layers, 0.8–0.9 for convolutional layers.

---

## 7. Weight Initialization

- Purpose: prevent vanishing/exploding signals at the start of training.  
- Common strategies:  
  - **Xavier/Glorot**: for tanh/sigmoid (keeps variance constant)  
  - **He/Kaiming**: for ReLU/Leaky ReLU (accounts for zeroing in ReLU)  
- Bad initialization → slow convergence, unstable training.

---

## 8. Data Augmentation

- Generates **new training samples** without changing labels.  
- Image: flip, rotate, crop, color jitter  
- Text: synonym replacement, paraphrasing, masking  
- Audio: pitch shift, time stretch, additive noise  
- Reduces overfitting, improves **generalization**, simulates **real-world variation**.

---

## 9. Overfitting in DL → Mitigations

- **Problem:** networks memorize training data due to high capacity.  
- **Solutions:**  
  - Dropout, L2 regularization (weight decay)  
  - Data augmentation  
  - Early stopping, reducing network size  
  - Label smoothing, mixup, CutMix  
- Validation monitoring crucial for hyperparameter tuning.

---

## 10. Vanishing & Exploding Gradients

- **Vanishing:** gradients shrink → early layers learn slowly.  
- **Exploding:** gradients grow → unstable updates.  
- **Mitigations:**  
  - Proper weight initialization  
  - ReLU-family activations  
  - Gradient clipping  
  - Residual connections (ResNet)  
  - BatchNorm / LayerNorm

---

## 11. Activation Functions

| Activation | Formula | Pros | Cons | Use Case |
|------------|---------|------|------|----------|
| ReLU | `max(0, x)` | Fast, sparse, avoids vanishing in hidden layers | Dead neurons if negative inputs dominate | Hidden layers |
| Leaky ReLU | `x if x>0 else 0.01*x` | Fixes dead neurons | Small negative slope may slow convergence | Hidden layers |
| Sigmoid | `1/(1+exp(-x))` | Probabilities (0–1) | Saturates → vanishing gradients | Output layer for binary classification |
| Tanh | `(exp(x)-exp(-x))/(exp(x)+exp(-x))` | Zero-centered | Saturation → vanishing gradients | Hidden layers (less used now) |
| Softmax | `exp(x_i)/sum(exp(x_j))` | Converts logits to probabilities | Sensitive to outliers | Output layer for multi-class |

---

## 12. BatchNorm & LayerNorm

- Normalize activations **within a batch (BatchNorm)** or **per sample (LayerNorm)**.  
- Reduces internal covariate shift, stabilizes training.  
- Allows **higher learning rates** and faster convergence.  
- Often used before or after activation.

---

## 13. Optimizers

| Optimizer | Key Idea | Pros | Cons |
|-----------|---------|------|------|
| SGD | Basic gradient descent | Simple, interpretable | Slow convergence, sensitive to LR |
| SGD+Momentum | Accumulates velocity | Faster convergence | Extra hyperparameter |
| RMSProp | Adaptive learning rate per param | Handles non-stationary objectives | Can overshoot minima |
| Adam | Combines momentum + RMSProp | Fast, robust, default choice | Sometimes poor generalization |

---

## 14. Loss Functions

- **Regression:** MSE, MAE, Huber  
- **Classification:** Cross-entropy, focal loss, KL divergence  
- **Ranking / Embeddings:** Triplet loss, contrastive loss  
- **Generative:** MSE, BCE, adversarial loss (GANs)

---

## 15. Architectures Overview

| Type | Description | Use Case |
|------|------------|---------|
| CNN | Convolution + pooling → translation invariance | Images, spatial data |
| RNN / LSTM / GRU | Sequential processing, memory | Text, time series, speech |
| Transformers | Self-attention, parallelizable | NLP, vision, multimodal |
| Autoencoder | Encode → decode latent representation | Dimensionality reduction, anomaly detection |
| GAN | Generator + Discriminator | Image generation, data augmentation |

---

## 16. Regularization & Generalization Tricks

- Early stopping  
- Dropout / DropConnect  
- Weight decay (L2)  
- Label smoothing  
- Data augmentation  
- Mixup / CutMix  
- Stochastic depth (for deep ResNets)

---

## 17. Practical Tips

- Use **pretrained models** for small datasets → transfer learning.  
- Monitor **train/val loss & metrics** to detect overfitting.  
- Always scale/normalize inputs; images often in [0,1] or [-1,1].  
- Check **activation distributions** to prevent dead neurons or saturation.

---
