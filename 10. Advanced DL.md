# Deep Learning & Transformers Cheat Sheet - Interview Notes

---

## 1. Residual Networks (ResNet / Skip Connections)
- **Formula:** `y = F(x) + x`  
- **Purpose:** Learn residuals → easier to train very deep networks (50-152+ layers)  
- **Advantages:**  
  - Better gradient flow → mitigates vanishing gradients  
  - Allows learning “small corrections” instead of full mapping  
- **Variants:** Bottleneck blocks (1×1 → 3×3 → 1×1), ResNeXt (parallel paths)

---

## 2. Gradient Flow & Vanishing Gradients
- Deep plain CNNs (e.g., VGG) → gradients vanish across layers  
- **ResNet fix:** identity shortcuts create direct paths:
```

∂L/∂x_l = ∂L/∂x_L * (1 + ∂F/∂x_l)

```
- Enables training deeper networks efficiently  

---

## 3. Flash Attention
- **Optimized attention** for memory efficiency  
- Computes softmax in **tiles / chunks** instead of full n×n matrix  
- Benefits: exact attention, faster training/inference, supports long sequences (10k+ tokens)  

---

## 4. Layer Hierarchy & Fine-Tuning
- **Early layers:** generic features (edges, textures)  
- **Middle layers:** shapes, motifs  
- **Late layers:** task-specific features  
- **Fine-tuning strategies:**  
- Freeze early layers → train top layers  
- Gradual unfreeze → adapt more layers  
- Use smaller LR for frozen layers, higher LR for top layers  

---

## 5. Saddle Points
- **Definition:** Gradient = 0, but point is neither max nor min  
- **Impact:** slow convergence in high-dimensional loss surfaces  
- **Escape:** stochasticity in SGD/Adam, momentum, noise  

---

## 6. Natural Gradient
- Rescales gradients using **Fisher Information Matrix**:
```

θ_{t+1} = θ_t − η F^{-1} ∇_θ L(θ)

```
- **Pros:** parameterization-invariant, efficient steps  
- **Cons:** expensive for large networks; approximations like K-FAC needed  

---

## 7. Entropy & Information Theory
- **Entropy:** uncertainty of a random variable
```

H(X) = −∑ p(x) log p(x)

```
- Applications:  
- Cross-entropy loss in classification  
- Decision tree splits (information gain)  
- Uncertainty estimation  

---

## 8. Label Smoothing
- Softens one-hot labels:
```

y_smooth = (1 − ε) * y_onehot + ε / C

```
- **Benefits:**  
- Reduces overconfidence → better probability calibration  
- Acts as regularization → improves generalization  

---

## 9. Positional Encoding (Transformers)
- **Absolute:** sinusoidal or learned embeddings added to tokens  
- **Relative:** encodes distance between tokens → generalizes better to long sequences  
- **RoPE:** rotates Q/K vectors → supports long context efficiently  

---

## 10. Self-Attention / QKV
- **Operation:** Weighted sum of values based on similarity of queries and keys  
- **Q/K/V:** Q/K = matching; V = content to pass  
- **Output:** learned, content-aware averaging
- **Masks:** padding mask, causal mask → enforce structural constraints  

---

## 11. Attention Optimizations
- **FlashAttention:** memory-efficient tiling  
- **Parameter sharing:** same weights across time/space → reduces parameters, improves generalization  
- **Multi-head attention:** multiple Q/K/V projections → capture diverse relationships  

---

## 12. Regularization Techniques
- **Dropout / DropConnect:** randomly drop activations or weights  
- **Stochastic depth:** randomly skip layers  
- **Data augmentations:** Mixup, CutMix, RandAugment  
- **Weight decay (L2):** prevents parameter explosion  
- **Label smoothing:** reduces overconfidence  

---

## 13. Optimizer Insights
- **SGD:** generalizable, slower  
- **Adam / AdamW:** adaptive LR, widely used in Transformers  
- **LARS / LAMB:** large-batch training  
- **Ranger / Lookahead:** combines adaptive + stabilizing features  

---

## 14. Practical Tips for Interviews
- Explain **residuals + skip connections** intuitively  
- Discuss **saddle points and optimizer behavior**  
- Show understanding of **memory-efficient attention**  
- Highlight **layer hierarchy → fine-tuning strategy**  
- Be familiar with **entropy, label smoothing, and regularization** for practical and theoretical discussion  
