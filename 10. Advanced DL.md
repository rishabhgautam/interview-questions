# Advanced Deep Learning - Interview Notes

---

## 1. Residual Networks (ResNet, Skip Connections)

- **Core Idea:** Each block learns a residual mapping:
```

y = F(x) + x

```
- F(x) = residual function learned by convolution layers  
- x = input to the block (identity shortcut)
- **Advantages:**  
- Eases training of **very deep networks** (50, 101, 152+ layers)  
- Improves gradient flow → mitigates vanishing gradients  
- Enables learning **small corrections** instead of full transformations
- **Variants:**  
- **Bottleneck blocks:** 1×1 → 3×3 → 1×1 conv to reduce parameters  
- **ResNeXt:** adds cardinality dimension (multiple parallel paths)  

---

## 2. How ResNet Fixes VGG’s Vanishing Gradient Problem

- Vanilla deep CNNs: gradient ∂L/∂x multiplied layer-by-layer → may vanish or explode  
- ResNet shortcut creates **direct path for gradient**:  
```

∂L/∂x_l = ∂L/∂x_L * (1 + ∂F/∂x_l)

```
- Gradient always has additive path → prevents vanishing  
- Allows **deeper networks** to converge efficiently without careful initialization  

---

## 3. Flash Attention

- **Memory-efficient attention** for Transformers  
- Instead of computing full n×n attention matrix:  
- Splits sequence into **tiles / blocks**  
- Computes softmax in chunks → reduces peak memory usage  
- **Advantages:**  
- Exact attention (no approximation)  
- Faster training and inference for long sequences  
- Exploits **GPU memory hierarchy** → avoids repeated DRAM loads  
- Useful for LLMs with sequences of **10k+ tokens**

---

## 4. Tall / Deep Layers and Fine-Tuning

- **Layer hierarchy in deep nets:**  
- Early layers: generic features (edges, textures)  
- Middle layers: combinations of features (shapes, motifs)  
- Late layers: task-specific features  
- **Fine-tuning strategies:**  
- Freeze early layers → train only later layers → avoid overfitting  
- Gradual unfreezing → progressively adapt more layers  
- Learning rate scheduling: smaller LR for early layers, higher LR for top layers  

---

## 5. Saddle Points

- High-dimensional loss surfaces: **more saddle points than local minima**  
- **Characteristics:**  
- Gradient = 0, but not a minimum  
- Curvature: positive in some directions, negative in others  
- **Optimization impact:**  
- SGD/Adam stochasticity helps **escape flat saddles**  
- Adding **momentum or noise** improves convergence through saddle regions  

---

## 6. Natural Gradient — Why It’s Hard

- **Standard gradient descent** ignores geometry of parameter space → can take inefficient steps  
- **Natural gradient:**  
```

θ_{t+1} = θ_t − η F^{-1} ∇_θ L(θ)

```
- F = Fisher Information Matrix → encodes curvature of parameter manifold  
- Pros: parameterization-invariant updates → faster convergence  
- Cons: computing/inverting F is **computationally expensive**  
- Practical approximations: **K-FAC**, **block-diagonal F**  

---

## 7. Entropy in Information Theory

- **Definition:** Average uncertainty of a discrete variable X:
```

H(X) = −∑_i p(x_i) log p(x_i)

````
- High entropy → unpredictable outcomes, low entropy → predictable  
- **Applications in ML:**  
- Cross-entropy loss in classification:  
  ```
  L = −∑_i y_i log(ŷ_i)
  ```
- Decision trees: entropy used for **information gain** to split nodes  
- Regularization / uncertainty estimation  

---

## 8. Label Smoothing

- Replace hard one-hot labels with “soft” targets:  
````

y_smooth = (1 − ε) * y_onehot + ε / C

```
- ε = smoothing factor, C = number of classes  
- Benefits:  
- Reduces overconfidence → better calibration  
- Acts as **regularization** → improves generalization  
- Common in **Transformers, image classification, and seq2seq models**  

---

## 9. Layer-wise Adaptive Rate Scaling (LARS) / LAMB

- Optimizers for **large-batch training**  
- **LARS:** scales learning rate for each layer based on **ratio of weight norm to gradient norm**  
- **LAMB:** combines LARS + Adam → stable training of billion-parameter models  

---

## 10. Gradient Clipping

- **Problem:** exploding gradients in RNNs / deep nets  
- **Solution:** clip gradient norm:
```

if ||g|| > threshold: g = g * threshold / ||g||

```
- Stabilizes training → prevents divergence  

---

## 11. BatchNorm / LayerNorm vs Residuals

- **BatchNorm:** normalizes across batch → faster convergence, reduces covariate shift  
- **LayerNorm:** normalizes across features → used in Transformers  
- **Residual connections:** complement normalization → improve gradient flow  
- **Takeaway:** combined, they allow **very deep, stable networks**  

---

## 12. Advanced Regularization Techniques

- **Dropout / DropConnect:** randomly drop activations or weights  
- **Stochastic depth:** randomly skip entire layers during training  
- **Data augmentation:** Mixup, CutMix, RandAugment  
- **Label smoothing:** reduces overconfidence  
- **Weight decay / L2 regularization:** prevents parameter explosion  

---

## 13. Optimizer Insights

- **SGD:** simple, generalizable, slow convergence  
- **Adam / AdamW:** adaptive learning rates, better for deep / transformer architectures  
- **Ranger / Lookahead:** combines adaptive + stabilizing features for faster convergence  

---

## 14. Practical Interview Takeaways

- Be able to **explain residuals + skip connections intuitively**  
- Discuss **why deep nets have saddle points** and how optimizers escape them  
- Know **memory-efficient attention tricks** like FlashAttention  
- Show understanding of **layer hierarchy → fine-tuning strategy**  
- Be familiar with **entropy, label smoothing, regularization** as theoretical + practical concepts  
