# Advanced Deep Learning - Interview Notes

---

## 1. Residual Networks (ResNet, Skip Connections)

ResNets add **skip (shortcut) connections** that let layers learn a *residual* function: F(x) = H(x) − x, so the block outputs x + F(x).  
This makes it easier to learn “small corrections” to the identity rather than the full mapping from scratch.  
Skip connections help gradients flow through many layers without vanishing completely.  
They enable training of very deep networks (50, 101, 152+ layers) that still optimize well.

---

## 2. How ResNet Fixes VGG’s Vanishing Gradient Problem

VGG-style networks are plain stacks of conv layers, so gradients must pass through every layer sequentially.  
In deep VGG nets, gradients fade (vanish) as they move backward, making early layers hard to train.  
ResNet’s identity shortcuts create **direct gradient paths** from deeper layers to earlier ones.  
This preserves gradient magnitude and allows effective training of much deeper architectures than VGG.

---

## 3. Flash Attention

FlashAttention is an optimized implementation of attention that is **IO-aware** and memory-efficient.  
Instead of forming the full attention matrix in memory, it uses tiling and fused kernels to compute softmax attention in chunks.  
This reduces memory complexity and often speeds up training/inference, especially for long sequences.  
It keeps exact attention (not approximate) but reorganizes computation to better exploit GPU memory hierarchy.

---

## 4. Tall / Deep Layers and Fine-Tuning

In deep nets, **upper (later) layers** are more task-specific, while **lower (earlier) layers** learn generic features.  
Fine-tuning often starts by updating only the **top layers**, keeping earlier layers frozen to avoid destroying useful generic representations.  
As you need more domain adaptation, you can progressively unfreeze lower layers.  
Deeper fine-tuning can improve performance but increases overfitting risk and compute.

---

## 5. Saddle Points

A saddle point is where the gradient is zero but the point is **not a local minimum** (minimum in some directions, maximum in others).  
High-dimensional loss surfaces (like deep nets) are full of saddle points rather than “bad” local minima.  
Optimization can get stuck or slow down near flat saddle regions because gradients are tiny.  
Stochastic optimizers (SGD, Adam) and noise from mini-batches help escape many saddle points.

---

## 6. Natural Gradient — Why It’s Hard

Natural gradient descent rescales gradients by the **geometry of the parameter space**, using the Fisher Information Matrix.  
This often gives better, more “parameterization-invariant” steps than standard gradient descent.  
However, computing and inverting the full Fisher matrix is extremely costly for large neural networks.  
Practical use requires approximations (e.g., K-FAC), which are complex and still expensive at deep-learning scale.

---

## 7. Entropy in Information Theory

Entropy measures the **average uncertainty** or “surprise” in a random variable’s outcomes.  
High entropy means outcomes are more unpredictable; low entropy means they are more predictable.  
In compression, entropy equals the theoretical lower bound on average bits per symbol.  
In ML, entropy and cross-entropy underpin classification losses and decision tree splitting criteria.

---

## 8. Label Smoothing

Label smoothing replaces hard one-hot labels (e.g., [0,0,1,0]) with slightly “soft” targets (e.g., [0.02,0.02,0.94,0.02]).  
This discourages the model from becoming **overconfident** and memorizing exact one-hot targets.  
It acts as regularization, often improving generalization and calibration of predicted probabilities.  
Commonly used in large-scale classification and sequence-to-sequence / Transformer models.

```

