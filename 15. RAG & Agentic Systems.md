# RAG & Agentic Systems - Interview Notes

---

## 1. Query Routing, Reframing, Expansion

- **Query routing:** Decide *where* to send a query (which tool, index, DB, agent, or model). Example: route billing queries to “billing KB” index, technical queries to “dev docs”.  
- **Query reframing:** Rewrite the user query to be clearer, more explicit, or more grounded in system terms (e.g., expand acronyms, add missing context).  
- **Query expansion:** Add related terms, synonyms, or constraints (e.g., “invoice” → “bill, statement, due date”) to improve recall in retrieval.  
- Together, they improve **precision (routing), clarity (reframing), and recall (expansion)** in RAG pipelines.

---

## 2. Code RAG — Chunking Strategy

- Chunk by **semantic units**: files → modules → classes → functions, not by arbitrary fixed tokens only.  
- Keep **function + docstring + key comments** together; avoid splitting definitions across chunks.  
- Include limited **context window** around a function (e.g., imports, related helpers, same class methods).  
- Store metadata: language, file path, repo, function name, docstring summary; use these for retrieval filters.  
- For retrieval, combine **keyword/code-aware embeddings** with BM25/lexical search for symbols and APIs.

---

## 3. SQL RAG — Chunking Strategy

- Chunk around **schema units**: per table (columns + types + constraints), views, stored procedures, UDFs.  
- Include **examples of typical queries** next to the tables they use (joins, filters, group-bys).  
- Maintain separate chunks for **business definitions** (e.g., what “active customer” means) vs technical schema.  
- Add metadata: database name, environment (prod/dev), domain (billing, orders, support), last updated time.  
- Retrieval often uses a combination of **schema-aware search (table/column names)** + embeddings over docs, comments, and examples.

---

## 4. Retrieval: Bi-encoder vs Cross-encoder

- **Bi-encoder:** Encodes query and documents independently into vectors; similarity via dot product or cosine.  
  - Pros: Fast, scalable, can use ANN/vector databases; good for large corpora.  
  - Cons: Weaker fine-grained interaction; may miss subtle relevance.  

- **Cross-encoder:** Concatenates query + document and runs them through the model jointly.  
  - Pros: Stronger relevance modeling via full token–token interaction.  
  - Cons: Slow and expensive; usually used for **re-ranking** top candidates from a bi-encoder.

---

## 5. How ColBERT Performs Late Interaction

- ColBERT uses a **bi-encoder style**: encodes queries and documents independently, but keeps **token-level embeddings**.  
- At retrieval, it performs **late interaction**: for each query token, compute similarity with all document token embeddings and take the **max** (MaxSim).  
- Document score = sum (or aggregate) of these per-query-token max similarities.  
- This preserves **fine-grained term interactions** while still allowing efficient indexing and ANN search on document token vectors.  
- It’s a middle ground between fast bi-encoders and expensive full cross-encoders.


