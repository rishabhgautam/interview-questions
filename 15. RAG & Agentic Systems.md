# RAG & Agentic Systems - Interview Notes

---

## 1. Query Routing, Reframing, Expansion

**Query routing**  
- Task: Decide **which system/agent/index/model** should handle a query.  
- Examples:  
  - Billing questions → “Billing KB” / billing agent  
  - Product how-to → “User guide KB”  
  - Code/API questions → “Dev docs / Code RAG index”  
- Signals:  
  - Keywords (e.g., “invoice”, “API key”, “timeout”)  
  - Metadata (user role, product, region)  
  - Simple classifier / router LLM (few-shot: “route_to = billing/dev_docs/general”).  
- Goal: Maximize **precision** by sending the query to the right knowledge pool.

**Query reframing**  
- Task: Rewrite the query into a **model-friendly, context-rich** form.  
- Examples:  
  - “bill not paid” → “Customer cannot pay the invoice due to card failure in checkout flow.”  
  - Add missing context: product name, user region, time window, environment (prod vs test).  
- Used for:  
  - Clarifying pronouns (“it”, “this feature”)  
  - Converting vague language into specific technical phrases  
  - Making the query self-contained (no hidden chat history assumptions).  

**Query expansion**  
- Task: Add **synonyms, related terms, entities, spelling variants** to boost recall.  
- Examples:  
  - “invoice” → {“bill”, “statement”, “payment due”, “billing cycle”}  
  - “SLA” → {“service level agreement”, “response time”, “uptime guarantee”}  
- Techniques:  
  - Lexical: synonym lists, domain thesaurus, ontology  
  - Embedding-based expansion using nearest neighbors  
  - LLM-based rewrite: “Generate 3 expanded queries with synonyms and related concepts.”  

**Mental model:**  
- Routing → *where?*  
- Reframing → *what exactly?*  
- Expansion → *what else is related?*  
→ Combined, they improve **precision (routing), clarity (reframing), and recall (expansion)** in RAG.

---

## 2. Code RAG — Chunking Strategy

**Chunking principles**  
- Prefer **semantic units** instead of blind fixed-length chunks. Think:  
  - File → module → class → function / method  
- Keep together:  
  - Function signature + docstring + key comments  
  - Related helper functions in the same region  
- Avoid splitting:  
  - A function definition across chunks  
  - Logic that spans a small, tightly coupled region (e.g., state machine / handler chain)

**Context & metadata**  
- Include limited **context window** around a function:  
  - Imports used by that function  
  - Class definition / base class  
  - Neighboring methods in same class/module  
- Store rich metadata:  
  - Language, file path, repo, branch, commit hash  
  - Symbol type (class/function/constant), module name  
  - Short summary: “What does this function do?”  

**Retrieval strategy**  
- Combine **lexical + embedding**:  
  - Lexical: search by symbol names, function names, class names, file paths  
  - Embeddings: semantic similarity for “what does this code do?” queries  
- Typical pipeline:  
  1. Retrieve by keyword/symbol (BM25 / keyword index)  
  2. Re-rank or enrich with code-aware embeddings  
  3. Optionally expand with related symbols (e.g., calls/callees from static analysis)

**Interview hooks:**  
- “Chunk by **logical units** (functions/classes) + add **metadata** + combine lexical + semantic retrieval.”

---

## 3. SQL RAG — Chunking Strategy

**Schema-centric chunking**  
- Chunk by **schema objects**:  
  - One chunk per table (name, columns, types, constraints, primary/foreign keys)  
  - One chunk per view, stored procedure, or materialized view  
  - One chunk per UDF / business logic block  
- Example table chunk:  
  - Table name, purpose, columns, data types, indexes, common join keys.

**Business definition vs technical schema**  
- Separate chunks for:  
  - **Business definitions:** what is an “active customer”, “churned account”, “billable event”?  
  - **Technical schema:** how those concepts map to tables & fields.  
- This separation helps LLMs resolve:  
  - “Active customer” → filter conditions (last_activity_date, status_flag, etc.)

**Query examples & patterns**  
- For each table or domain, store:  
  - Common **join patterns** (e.g., customer ↔ orders ↔ payments)  
  - Example queries (aggregations, filters, time windows)  
  - Performance tips (indexes, partitions)  

**Metadata & retrieval**  
- Metadata: database name, environment (prod/dev/test), domain (billing, orders, support), last updated.  
- Retrieval:  
  - First retrieve schema + definitions relevant to mentioned tables/columns/metrics  
  - Then retrieve example queries and patterns for SQL generation  
- Pipeline: “User question → identify entities/metrics → retrieve schema chunks + examples → generate SQL with constraints.”

**Interview hook:**  
- “Chunk by **tables/views/procs**, store **business definitions + examples**, then retrieve both schema + patterns for SQL generation.”

---

## 4. Retrieval: Bi-encoder vs Cross-encoder

**Bi-encoder (dual encoder)**  
- Query and documents encoded **separately** into vectors; similarity via dot product / cosine.  
- Pros:  
  - Fast retrieval using ANN/vector indices  
  - Scales to **millions/billions** of docs  
  - Great for first-stage retrieval in RAG  
- Cons:  
  - Weaker token-level interaction → misses subtle relevance cues  
  - Needs careful training to handle domain-specific language

**Cross-encoder**  
- Query and document are **concatenated** and passed through one model jointly.  
- Pros:  
  - Full **token–token interaction** → very accurate relevance ranking  
  - Great as a **re-ranker** on top-k results from bi-encoder  
- Cons:  
  - Much **slower and more expensive** → not scalable as first-pass retrieval  
  - Typically limited to a small candidate set (e.g., top 50–200 docs)

**Typical RAG pattern**  
1. Bi-encoder retrieves top N candidates (fast).  
2. Cross-encoder re-ranks these N candidates (accurate).  
3. Top K from re-ranked list are passed into the LLM context.  

**Interview hook:**  
- “Bi-encoder = **fast & scalable**; cross-encoder = **slow but accurate** → use bi-encoder for retrieval, cross-encoder for re-ranking.”

---

## 5. How ColBERT Performs Late Interaction

**Core idea**  
- ColBERT = **CO**ntextualized **L**ate **BERT** interaction.  
- It’s a **bi-encoder style** method that keeps **per-token embeddings** instead of a single pooled embedding.  

**Encoding**  
- Query: encode tokens → get embeddings \( Q = [q_1, q_2, ..., q_m] \)  
- Document: encode tokens → get embeddings \( D = [d_1, d_2, ..., d_n] \)  
- Document tokens and query tokens are both **contextualized** by BERT/transformer.

**Late interaction (MaxSim)**  
- For each query token embedding \(q_i\):  
  - Compute similarity with all document token embeddings \(d_j\).  
  - Take the **maximum** similarity:  
    \[
    \text{score}(q_i, D) = \max_j \cos(q_i, d_j)
    \]  
- The final doc score is the **sum (or aggregate)** of all query-token scores:  
  \[
  \text{score}(Q, D) = \sum_i \text{score}(q_i, D)
  \]  

**Why it’s powerful**  
- Preserves **fine-grained term-level matching** (like cross-encoder) but still:  
  - Documents are pre-encoded once and stored token-wise.  
  - Retrieval uses efficient ANN search over token embeddings.  
- Middle ground between:  
  - **Bi-encoder:** single vector per doc, fast but coarse  
  - **Cross-encoder:** full joint encoding, slow but detailed  

**Mental model:**  
- “Store all **word-level vectors** for documents, then at query time, let each query word find its **best matching doc word** and sum up those matches.”  

**Typical use:**  
- Web/doc search where **semantic but precise** matching is needed:  
  - Good for long documents  
  - Good for keyword-sensitive scenarios (code search, legal, technical docs).

---

## Quick Interview Summary (RAG & Agentic)

- **Routing:** choose right index/agent → increases precision.  
- **Reframing/Expansion:** clarify + broaden query → improves retrieval quality.  
- **Code RAG:** semantic chunks at **function/class level**, rich metadata, hybrid lexical+embedding search.  
- **SQL RAG:** chunk by **tables/views/procs** + business definitions + example queries; retrieve both schema & patterns.  
- **Bi-encoder vs Cross-encoder:** **fast retrieval vs precise re-ranking**.  
- **ColBERT:** late interaction → **token-level matching + scalable indexing**.
