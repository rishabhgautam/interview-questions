# Popular Machine Learning Algorithms — Interview Notes

---

## Linear Regression — Pros, Cons, Assumptions

**Pros:** Simple, interpretable, fast to train, good baseline for regression tasks.  
**Cons:** Performs poorly with strong non-linear relationships or heavy multicollinearity; sensitive to outliers.  
**Assumptions:** Linearity, independence of errors, homoscedasticity (constant variance), no/low multicollinearity, residuals approximately normal.  
Works best when relationships are roughly linear and features are well-engineered.

---

## Logistic Regression — Pros, Cons, Why “Linear”?

**Pros:** Interpretable coefficients (log-odds), fast, well-calibrated probabilities, robust with regularization.  
**Cons:** Assumes linear decision boundary in feature space; struggles with complex non-linear patterns unless features are engineered.  
**Why linear?** Decision boundary is defined by \( w^T x + b = 0 \), which is a hyperplane → linear classifier.  
Non-linearity comes only from the sigmoid mapping scores to probabilities.

---

## Support Vector Machines (SVM) — Pros, Cons, Kernel Trick

**Pros:** Effective in high-dimensional spaces, maximizes margin, robust to overfitting with proper C and kernel.  
**Cons:** Slow on very large datasets, sensitive to choice of kernel and hyperparameters, less interpretable.  
**Kernel trick:** Implicitly maps data into a higher-dimensional feature space using kernels (RBF, polynomial) without computing the mapping explicitly.  
This allows learning non-linear decision boundaries using a linear separator in the transformed space.

---

## Decision Trees — Pros, Cons, Cost Functions, Splits

**Pros:** Easy to interpret and visualize, handles numeric & categorical features, non-linear boundaries, little preprocessing.  
**Cons:** High variance, prone to overfitting, small changes in data can yield very different trees.  
**Cost functions:** Gini impurity or entropy for classification; MSE for regression.  
**Splits:** Numeric features are split at thresholds (e.g., \(x < t\)); categorical features are split by grouping categories into subsets.

---

## Random Forest — Pros, Cons

**Pros:** Reduces variance via averaging many trees, handles non-linearities, robust to overfitting and noise, works well out-of-the-box.  
**Cons:** Less interpretable than a single tree, can be slow and memory-heavy for many trees and features.  
Good default choice for tabular data, especially when you don’t want heavy feature engineering.

---

## K-Nearest Neighbors (KNN) — Pros, Cons, Distance-Weighted KNN

**Pros:** Very simple, non-parametric, adapts to complex decision boundaries, no training time.  
**Cons:** Prediction is slow for large datasets, sensitive to irrelevant features and feature scaling, memory-intensive.  
**Distance-weighted KNN:** Neighbors closer to the query point get higher weight (e.g., 1/distance), improving robustness.  
Works best on low-dimensional, well-scaled data.

---

## K-means — Pros, Cons, Convergence, Empty Clusters

**Pros:** Simple, fast, scalable, widely used for clustering with spherical clusters.  
**Cons:** Assumes convex, roughly equal-sized clusters; sensitive to initialization and outliers; needs K specified.  
**Convergence:** Guaranteed to converge in finite steps to a local minimum of within-cluster variance (but not global).  
**Empty clusters:** Can occur when no points are assigned to a centroid; often handled by reinitializing centroid or merging.

---

## PCA — Pros, Cons

**Pros:** Reduces dimensionality, decorrelates features, often improves speed and reduces noise; helpful for visualization.  
**Cons:** Linear method; may miss non-linear structure; components are less interpretable; sensitive to scaling.  
Works best when variance directions are meaningful and you’re okay losing direct feature semantics.

---

## Naive Bayes — Pros, Cons, Independence Assumption

**Pros:** Extremely fast, low data requirement, works surprisingly well in text classification and high-dimensional settings.  
**Cons:** Strong independence assumption rarely holds; probability estimates can be poorly calibrated.  
**Independence assumption:** Features are conditionally independent given the class label, which simplifies \( P(x|y) \).  
Despite being “naive”, often a strong baseline.

---

## AdaBoost — Pros, Cons, Process

**Pros:** Focuses on hard-to-classify samples, can turn weak learners (e.g., decision stumps) into a strong ensemble, often achieves high accuracy on tabular data.  
**Cons:** Sensitive to noisy labels and outliers, can overfit if too many rounds or overly complex base learners are used.  

**Process (High-level):**  
- Start with **uniform weights** over all training samples.  
- Train a weak learner; compute its weighted error.  
- Assign a **learner weight** (alpha) based on its error: better learners get higher alpha.  
- **Increase weights** of misclassified samples and **decrease weights** of correctly classified ones so the next learner focuses on “hard” points.  
- Final prediction is a **weighted vote** of all weak learners (sign of weighted sum in classification).

---

## Gradient Boosting vs Random Forest — Differences

**Random Forest:** Bagging of trees; trees trained independently on bootstrapped samples; reduces variance.  
**Gradient Boosting:** Trees added sequentially; each new tree fits residuals of previous ensemble; reduces bias.  
RF is more robust, easier to tune, less prone to overfitting; GBM often achieves higher accuracy but needs careful tuning.  
In short: RF = parallel, variance reduction; GBM = sequential, bias reduction.

---

## GBDT — Pros, Cons, Sparse Data Handling

**Pros:** State-of-the-art on many tabular tasks, handles non-linear interactions, supports various loss functions.  
**Cons:** Sensitive to hyperparameters, prone to overfitting if not regularized, slower to train than single models.  
**Sparse data:** Implementations like XGBoost/LightGBM efficiently handle sparse matrices with optimized split finding and missing-value handling.  
Great for structured data when tuned well.

---

## Why GBDTs Are Inefficient for Online Learning

GBDTs build trees sequentially, where each new tree corrects residual errors of all previous trees.  
Updating the model with new data would require recomputing residuals and potentially rebuilding many trees.  
Trees themselves are not naturally incremental, unlike linear models or online algorithms.  
As a result, GBDTs are typically retrained in batches rather than updated per-sample.

---

## Ensemble Methods — Types & Why They Help

**Types:** Bagging, boosting, stacking, and voting.  
They combine multiple models to reduce variance, reduce bias, or exploit different model strengths.  
Bagging averages diverse high-variance models; boosting sequentially corrects errors; stacking learns an optimal way to combine base models.  
Overall, ensembles usually generalize better than individual models.

---

## Bagging, Boosting, XGBoost

**Bagging:** Train models on bootstrapped samples and average their predictions (e.g., Random Forest); mainly reduces variance.  
**Boosting:** Sequentially add weak learners that focus on previous mistakes (e.g., AdaBoost, GBDT); mainly reduces bias.  
**XGBoost:** Highly optimized GBDT with regularization, efficient splits, handling of missing values, and system-level optimizations.  
XGBoost is often the go-to GBDT library in competitions and production.

---

## Autoencoder vs Variational Autoencoder (VAE)

**Autoencoder (AE)**  
- Has an encoder that maps input `x` to a latent vector `z`, and a decoder that reconstructs `x_hat` from `z`.  
- Trained to minimize reconstruction loss between `x` and `x_hat` (e.g., MSE or cross-entropy).  
- Learns a **deterministic** latent representation (same `x` → same `z`), good for compression, denoising, and dimensionality reduction.  
- Not inherently probabilistic: you cannot reliably sample new realistic points just by picking random `z`.

**Variational Autoencoder (VAE)**  
- Encoder outputs parameters of a **distribution over z** (typically mean and log-variance), instead of a single vector.  
- Uses the **reparameterization trick**: sample `epsilon` from a standard normal and compute `z = mu + sigma * epsilon` so gradients can flow through.  
- Loss = reconstruction loss **+** KL-divergence term that regularizes the latent distribution towards a simple prior (usually standard normal).  
- Because the latent space is smooth and continuous, you can sample `z` from the prior and decode it to **generate new data**, making VAEs proper generative models.

---

## EM Algorithm (Expectation-Maximization)

EM is an iterative algorithm to find maximum likelihood estimates when data has hidden/latent variables.  
**E-step:** Compute expected values of hidden variables given current parameters.  
**M-step:** Maximize the expected complete-data log-likelihood to update parameters.  
Used in mixtures of Gaussians, HMMs, and other latent variable models.

---

## K-means: How to Choose K?

Common methods include the **elbow method** (look for a “knee” in SSE vs K curve) and **silhouette score** (choose K with highest average silhouette).  
Information criteria like BIC/AIC can also guide K when modeling assumptions hold.  
Pragmatically, choose K based on domain knowledge, interpretability, and downstream use.  
Often you try several K values and compare stability and usefulness of clusters.


