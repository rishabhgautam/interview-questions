# **ML Foundations - Interview Notes**

---

## **1. L1 vs L2 Regularization**

- **L1 (Lasso):** Adds `λ * |w|` → pushes some weights **exactly to zero** → built-in feature selection.  
- **L2 (Ridge):** Adds `λ * w²` → shrinks all weights smoothly, spreads influence across features.  
- **When to use:** L1 for **sparse models / many irrelevant features**, L2 for **stability & multicollinearity**.  
- **Optimization:** L2 is smoother and easier to optimize; L1 has kinks at 0, but gives interpretability.

---

## **2. Bias, Variance & the Trade-off**

- **Bias:** Error from **too-simple assumptions** (underfitting).  
- **Variance:** Error from **too much sensitivity** to training data (overfitting).  
- **Trade-off:** Increasing model complexity ↓ bias but ↑ variance; simplifying does the opposite.  
- **Mental model:** Total error ≈ Bias² + Variance + Irreducible noise → we tune model to hit the “sweet spot”.

---

## **3. Overfitting & How to Prevent It**

- **Signs:** Very high train performance, much lower test/validation performance.  
- **Causes:** Model too complex, weak regularization, small/dirty data.  
- **Fix levers:** Regularization (L1/L2), dropout, early stopping, simpler models, data augmentation, more data.  
- **Mind map:** “Too much memory” → make model **simpler**, data **richer**, training **more constrained**.

---

## **4. Underfitting**

- **Signs:** Both train and test scores are **consistently poor**.  
- **Causes:** Model too simple, not enough training, poor feature set, too strong regularization.  
- **Fix levers:** Increase model capacity, add non-linear features, reduce regularization, train longer.  
- **Mental image:** Model is “blind / deaf” → give it more **capacity** and better **features**.

---

## **5. Cross-validation**

- **Goal:** Get a **reliable estimate** of generalization by training/validating on multiple splits.  
- **K-fold CV:** Split into K folds; each fold becomes validation once, others are training.  
- **Benefits:** Reduces dependence on a single lucky/unlucky split; good for **hyperparameter tuning**.  
- **Mind map:** “Rotate the validation set” → average performance over all rotations.

---

## **6. Gradient Descent + Local vs Global Minima**

- **Gradient Descent:** Move parameters **opposite to the gradient** to reduce loss.  
- **Non-convex loss (NNs):** Many local minima & saddle points; no global optimality guarantee.  
- **Global vs local:** Global = absolute lowest loss; local = low but not necessarily best.  
- **Practice:** SGD/Adam + noise + good init usually find **good-enough basins**, not necessarily global minima.

---

## **7. Batch vs Stochastic vs Mini-batch Gradient Descent**

- **Batch GD:** Uses full dataset per step → stable, accurate gradient but slow & memory heavy.  
- **Stochastic GD:** One sample per step → very noisy, but can help escape bad minima/saddle points.  
- **Mini-batch GD:** Small batches (e.g., 32–512) → balance of **speed, stability, and GPU use**.  
- **Default:** Mini-batch is the standard in modern deep learning.

---

## **8. Batch Normalization**

- **What it does:** Normalizes activations per mini-batch to zero mean, unit variance, then applies learnable γ and β.  
- **Benefits:** More stable training, allows higher learning rates, reduces internal covariate shift.  
- **Side effect:** Acts as mild regularizer due to mini-batch noise.  
- **Mental hook:** “Normalize, then re-scale” inside the network to keep activations well-behaved.

---

## **9. Active Learning**

- **Idea:** Model **asks** for labels on the most informative unlabeled samples.  
- **Strategies:** Uncertainty sampling, query-by-committee, diversity/coverage-based selection.  
- **When useful:** Labels expensive, unlabeled data plentiful (e.g., medical, legal, speech).  
- **Vs Online:** Online = sequential processing of all samples; Active = **selective labeling**.

---

## **10. Weak Supervision**

- **Idea:** Use **noisy / heuristic labels** instead of fully manual labeling.  
- **Sources:** Rules, pattern matching, distant supervision, noisy crowdsourcing, label functions.  
- **Key step:** Combine multiple weak signals with a **label model** (aggregation/denoising).  
- **Benefit:** Quickly create large training sets when gold labels are rare.

---

## **11. Selection Bias**

- **Definition:** Training data **not representative** of the population you care about.  
- **Examples:** Only early adopters, only one geography, only users who responded.  
- **Consequences:** Biased models, poor generalization, unfair decisions.  
- **Prevention:** Random/stratified sampling, careful inclusion criteria, continuous drift monitoring.

---

## **12. Handling Imbalanced Data**

- **Problem:** Minority class has **too few samples**; accuracy becomes misleading.  
- **Data-level fixes:** Oversampling (SMOTE), undersampling majority, synthetic generation.  
- **Algorithm-level fixes:** Class-weighted loss, focal loss, threshold tuning.  
- **Evaluation:** Prefer F1, AUC-PR, per-class recall over raw accuracy.

---

## **13. Fixing Distribution Shift**

- **Definition:** Train and test data follow **different distributions** (covariate, label, or concept shift).  
- **Symptoms:** Model performance degrades with time or across domains.  
- **Mitigations:** Domain adaptation, importance reweighting, robust features, retrain on fresh/target data.  
- **Production view:** Always track drift → trigger retraining / model updates.

---

## **14. Dummy Variable Trap**

- **What:** Including **all** dummy columns for a categorical variable → perfect multicollinearity.  
- **Fix:** Drop one category (baseline) so others are interpreted **relative** to it.  
- **Example:** For colors {Red, Blue, Green}, use only 2 dummies (e.g., Red, Blue).  
- **Shortcut:** Most libraries have `drop_first=True` or similar.

---

## **15. Supervised vs Unsupervised Learning**

- **Supervised:** Learn mapping **x → y** with labels (classification, regression).  
- **Unsupervised:** Discover structure in **x only** (clustering, PCA, anomaly detection).  
- **Goal:** Supervised → prediction; Unsupervised → grouping, compression, pattern discovery.  
- **Hybrids:** Semi-supervised, self-supervised combine both worlds.

---

## **16. Decision Boundary vs Decision Surface**

- **Decision boundary:** 2D curve/line separating classes in feature space.  
- **Decision surface:** Same concept in higher dimensions (hyperplane/hypersurface).  
- **Linear models:** Straight lines/planes; non-linear models: curved/complex surfaces.  
- **Mental image:** Color the space by predicted class → the “edges” between colors are the boundary/surface.

---

## **17. Generative vs Discriminative Models**

- **Generative:** Model **P(x, y)** or **P(x | y)** → can sample synthetic x for given y (e.g., Naive Bayes, GMMs, GANs).  
- **Discriminative:** Model **P(y | x)** or direct decision boundary (e.g., Logistic Regression, SVM, many NNs).  
- **Trade-off:** Generative = more assumptions, fewer labels; Discriminative = often better predictive performance with enough data.  
- **Hook:** “Generate the data” vs “Draw the boundary”.

---

## **18. Why MSE is Not Ideal for Classification**

- **Issue:** MSE assumes Gaussian noise, not Bernoulli/Categorical, and gives weak gradients near extremes.  
- **Misalignment:** Doesn’t match the true **log-likelihood** of discrete outputs.  
- **Better choice:** Cross-entropy directly optimizes log-likelihood for classification.  
- **Result:** Faster convergence, better calibrated probabilities with cross-entropy.

---

## **19. When Decision Trees Are Not Useful**

- **High-dimensional sparse data:** Trees don’t exploit linear structure as efficiently as linear models.  
- **Very smooth decision boundaries:** Linear/GLM models can be more compact and stable.  
- **Overfitting risk:** Single trees are high-variance without pruning or ensembling.  
- **Mental rule:** Use trees/ensembles for **tabular, mixed-type data**; avoid bare trees for text/embeddings.

---

## **20. Train / Validation / Test Split**

- **Train:** Learn model parameters.  
- **Validation:** Tune hyperparameters and make model-selection decisions.  
- **Test:** Final, **untouched** estimate of performance; used only at the end.  
- **Key rule:** Never tune on test; otherwise you “leak” information and overestimate performance.

---

## **21. Evaluation Metrics — Accuracy vs Precision, Recall, F1**

- **Accuracy:** (TP + TN) / all → can be misleading on imbalanced data.  
- **Precision:** TP / (TP + FP) → “When model predicts positive, how often is it correct?”.  
- **Recall:** TP / (TP + FN) → “Of all true positives, how many did we catch?”.  
- **F1-score:** Harmonic mean of precision & recall; good single metric when classes are imbalanced.  
- **Mind map:**  
  - Accuracy → overall  
  - Precision → *trust my positives*  
  - Recall → *catch all positives*  
  - F1 → balance precision & recall.
