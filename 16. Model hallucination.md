# Model Hallucination - Interview Notes

---

## 1. Identifying Hallucinations

### What is hallucination?

Hallucination is when a model produces **confident but incorrect, fabricated, or unverifiable** information.  
Examples: non-existent APIs, fake citations, imaginary research papers, wrong configuration options that “sound” plausible.

### Common patterns

- **Invented specifics**  
  - Fake library functions, error codes, endpoints, file paths.  
  - “You can use `customer_id_v3_hash` column” when no such column exists.

- **Internal contradictions**  
  - Different answers to the *same* question across turns.  
  - Inconsistency within a single answer (e.g., claims 95% accuracy in one bullet, 80% in another).

- **Overconfident tone**  
  - Uses strong language (“always”, “guaranteed”, “official”) without mentioning uncertainty or assumptions.  
  - No hedging even when the question is niche or post-knowledge-cutoff.

- **Unverifiable details**  
  - Very specific names, dates, or metrics that can’t be found in official docs, repos, or standards.  
  - Citations to papers or RFCs that don’t exist or don’t match the description.

- **Fabricated structure**  
  - Invented folder structures, API versions, CLI flags, or configuration keys that look “standard” but are wrong.

### Practical detection methods

- **External verification**  
  - Cross-check key facts against:  
    - Official docs (libraries, cloud services, frameworks)  
    - Authoritative sites (standards, RFCs, vendor docs)  
    - Internal source of truth (schema registry, API spec, config repo)  

- **Consistency checks**  
  - Ask the *same* question in different ways.  
  - Ask the model to restate its own answer more concisely and see if key details change.  

- **Ask for sources / reasoning**  
  - “Which official doc / section does this come from?”  
  - “List assumptions you are making.”  
  - Vague, generic, or clearly misaligned sources → red flag.  

- **Boundary / scope checks**  
  - Questions about **post-cutoff events**, private internal systems, or extremely niche topics are high-risk.  
  - Treat any detailed, confident answer there as suspect unless tool-verified.

- **Human-in-the-loop review**  
  - For **medical, legal, finance, compliance, safety**: always have expert review.  
  - Use LLM as a *drafting* or *brainstorming* tool, not final authority.

### Mental hook

> **Hallucination = confident + specific + unsupported.**  
> If it “sounds right” but you can’t trace it, treat it as unverified.

---

## 2. Mitigation Strategies

You can tackle hallucinations at **four layers**: prompt/UX, retrieval, decoding/guardrails, and verification/training.

---

### 2.1 Prompt & UX Design

- **Encourage uncertainty explicitly**
  - Add instructions like:  
    - “If you are not sure, say you don’t know.”  
    - “If information is missing, ask for clarification or state assumptions.”

- **Ask for sources or support**
  - “For each factual claim, mention whether it comes from context, prior knowledge, or is an educated guess.”  
  - “Cite docs / URLs or say ‘no source’.”

- **Constrain the task**
  - Instead of “Explain X”, use:  
    - “Summarize only from the given passage.”  
    - “Answer using only the provided context. If not enough, say so.”

- **Structured outputs**
  - Force the model to reveal uncertainty:  
    ```json
    {
      "answer": "...",
      "confidence": "low|medium|high",
      "supported_by_context": true,
      "needs_human_review": true
    }
    ```

- **User-facing UX**
  - Show a “⚠️ may be inaccurate” tag for free-form generations.  
  - Provide an explicit “Verify with docs” or “View sources” button.

**Interview hook:**  
*“We reduce hallucinations by asking the model to admit uncertainty, constrain its scope, and expose confidence & sources in the output.”*

---

### 2.2 Retrieval-Augmented Generation (RAG)

- **Ground answers in retrieved context**
  - Pipeline: *query → retrieve → answer from context*.  
  - System prompt: “Use only the provided context; if not enough, say so.”

- **Chunk + cite**
  - Attach a source ID / URL to each retrieved chunk.  
  - Ask the model: “For each key statement, reference the chunk ID that supports it.”  
  - If a statement has no supporting chunk → likely hallucinated.

- **Context checking step**
  - Add an internal check:  
    - “Does your answer strictly follow the context? Mark any part that is speculation.”  
  - Ask the model to output:  
    - `supported_parts`, `unsupported_parts`.

**Interview hook:**  
*“RAG turns ‘free imagination’ into ‘open-book exam’—the model should quote the book, not invent facts.”*

---

### 2.3 Constrained Decoding & Guardrails

- **Constrained generation**
  - For SQL/code/config: use grammars or schema-aware decoders so the model can only emit **valid structures**.  
  - Example: SQL generation constrained by actual table/column schema.

- **Guardrail policies**
  - Regex and policy checks for:  
    - Fake URLs / emails  
    - Disallowed content  
    - Sensitive operations (deletion, privilege escalation)  

- **Safety / validation layers**
  - Code: static analyzers, linters, test suites.  
  - SQL: run against read-only replicas or EXPLAIN-only mode first.  
  - Config: JSON/YAML schema validation before applying.

**Interview hook:**  
*“We box the model in: it must speak a valid grammar and pass validators before we trust the output.”*

---

### 2.4 Post-Generation Verification

- **Tool-augmented verification**
  - After generation, automatically call:  
    - Web/doc search APIs  
    - Internal schema / config / feature flag services  
    - Test or dry-run endpoints  
  - Compare facts (e.g., column names, endpoints, status codes) with the source of truth and patch mistakes.

- **Self-check pass**
  - Second prompt: “Review this answer. List any statements that might be wrong or unsupported and correct them.”  
  - Use a separate “critic” or “checker” model if needed.

- **Consistency enforcement**
  - Cache critical facts per session (e.g., definition of a field, a config value).  
  - On new answers, check: “Are you contradicting previously agreed facts?”

**Interview hook:**  
*“We don’t stop at generation; we verify post-hoc using tools and a second self-critique step.”*

---

### 2.5 Training-Time Approaches

- **Supervised fine-tuning with ‘I don’t know’**
  - Include examples where the *correct* answer is: “I don’t know / not in context / cannot answer safely.”  
  - Penalize fabricated confident answers in those cases.

- **RLHF / preference optimization**
  - Reward responses that are:  
    - Honest about uncertainty  
    - Explicit about assumptions  
    - Grounded in provided context  
  - Penalize answers that confidently invent facts, especially when context is insufficient.

- **Domain specialization**
  - Fine-tune on **clean, curated** domain data (official docs, well-reviewed content).  
  - Reduce reliance on generic web priors that may be noisy or outdated.

**Interview hook:**  
*“We teach the model that ‘I don’t know’ is better than a confident lie, especially in high-stakes domains.”*

---

### 2.6 System & Product-Level Mitigations

- **Disclaimers & UX cues**
  - Show that the system is **assistive**, not authoritative.  
  - Emphasize: “Verify critical outputs before acting.”

- **Trust tiers**
  - Tag outputs as:  
    - `RAW_LLM` (no verification)  
    - `CONTEXT_GROUNDED` (from RAG)  
    - `TOOL_VERIFIED` (cross-checked with APIs/docs)  
    - `HUMAN_REVIEWED`

- **Scope restriction**
  - For high-risk workflows:  
    - Restrict to **summarization, classification, retrieval**.  
    - Avoid free-form generative steps that might fabricate instructions or facts.

- **Audit & logging**
  - Log prompts, contexts, and outputs for auditing.  
  - Use logs to find patterns of hallucination and feed them back into training/evaluation.

**Interview hook:**  
*“We design the whole product around layered trust: from raw generations to verified and human-reviewed answers.”*

---

## 3. Quick 2–3 Line Answers for Interviews

- **Q: What is hallucination in LLMs?**  
  A: When the model produces **confident but incorrect or fabricated** information that isn’t grounded in facts or context.

- **Q: How do you reduce hallucinations in a production system?**  
  A: Use **RAG to ground answers in retrieved docs**, encourage uncertainty in prompts, add **guardrails and structured outputs**, and perform **post-generation verification** with tools and human review.

- **Q: Why can’t we fully eliminate hallucinations?**  
  A: LLMs are **probabilistic pattern learners**, not truth engines. They don’t have guaranteed access to ground truth, so we must treat them as **assistants plus verification layer**, not as authoritative sources.
