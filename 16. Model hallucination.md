# Model Hallucination - Interview Notes

---

## 1. Identifying Hallucinations

**What is hallucination?**  
Hallucination is when a model produces **confident but incorrect, fabricated, or unverifiable** information (e.g., fake citations, non-existent APIs, wrong facts).

**Common patterns:**
- **Invented specifics:** Non-existent paper titles, library functions, APIs, or error codes that “sound right” but don’t exist.  
- **Contradictions:** Different answers to the *same* question across turns, or contradictions within a single response.  
- **Overconfident tone:** Strong, definitive language (“always”, “guaranteed”, “official”) despite no evidence or disclaimers.  
- **Unverifiable details:** Names, dates, statistics that cannot be found in reliable external sources.  
- **Fabricated citations:** References to papers, URLs, or books that don’t exist or don’t match the described content.

**Practical detection methods:**
- **External verification:**  
  Cross-check key claims against documentation, databases, or trusted web sources (docs, standards, official repos).  
- **Consistency checks:**  
  Ask the model the *same* question in multiple ways; hallucinations often lead to inconsistent answers.  
- **Ask for reasoning or sources:**  
  Ask: “What is your source?” or “Cite the standard / RFC / official docs.” If answer is vague or contradictory, treat with suspicion.  
- **Boundary checks:**  
  Watch for answers outside the model’s “likely knowledge” (e.g., post-cutoff events, niche proprietary systems).  
- **Human-in-the-loop review:**  
  For high-stakes domains (medical, legal, finance), always have a human expert review outputs for correctness and safety.

---

## 2. Mitigation Strategies

### 1) Prompt & UX Design

- **Encourage uncertainty:**  
  Explicitly tell the model: “If you are not sure, say you don’t know or recommend verification.”  
- **Ask for citations:**  
  “Back each factual claim with a source or explain that it is an educated guess.” This pressure surfaces uncertainty.  
- **Constrain the task:**  
  Narrow the scope (e.g., “summarize the given text only” vs “summarize this topic from the internet”).  
- **Structured outputs:**  
  Request JSON with explicit fields like `{"answer": "...", "confidence": "low/medium/high", "needs_verification": true}`.

### 2) Retrieval-Augmented Generation (RAG)

- **Ground answers in retrieved context:**  
  Retrieve relevant documents and instruct the model: “Answer using only the provided context; if missing, say ‘not in context’.”  
- **Chunk + cite:**  
  For each sentence, require a pointer to a retrieved chunk (doc id, section, URL). Hallucinations often show up as “no matching chunk”.  
- **Context checking:**  
  Add checks like: “Is your answer fully supported by the provided context? If not, highlight missing pieces.”  

### 3) Constrained Decoding & Guardrails

- **Constrained generation:**  
  For tasks like code completion or SQL, restrict output to valid syntax/grammar (via constrained decoding, PEG grammars, or DSLs).  
- **Whitelist / blacklist patterns:**  
  Use regex or policies to block certain outputs (e.g., invented URLs, sensitive instructions).  
- **Safety / validation layers:**  
  Run outputs through validators: schema validators, static analyzers, or rule-based safety checks before returning to the user.

### 4) Post-Generation Verification

- **Tool-augmented verification:**  
  After generating an answer, call tools (web/docs APIs, DB queries) to verify key facts and correct them if needed.  
- **Self-checking step:**  
  Add an explicit second pass: “Review your previous answer, list any facts that might be wrong, and correct them.”  
- **Consistency enforcement:**  
  Cache key answers (e.g., definitions, config details) and reconcile future responses against this cache.

### 5) Training-Time Approaches

- **Supervised fine-tuning on “don’t know”:**  
  Add training examples where the correct behavior is to decline and say “I don’t know / cannot answer safely.”  
- **RLHF / preference tuning:**  
  Reward helpful, honest, cautious responses and penalize confident hallucinations.  
- **Domain specialization:**  
  Fine-tune on high-quality domain data (e.g., official docs) to reduce reliance on weak priors.

### 6) System & Product-Level Mitigations

- **Clear disclaimers & UX cues:**  
  Show that answers may be probabilistic, and encourage users to verify important information.  
- **Tiers of trust:**  
  Mark outputs as “unverified”, “tool-verified”, or “human-reviewed” depending on the pipeline used.  
- **Scope restriction:**  
  For high-risk workflows, limit the model to **summarization, classification, retrieval** rather than free-form generation.

---

**Key takeaway:**  
Hallucinations are not fully avoidable, but you can **detect, reduce, and contain** them via:  
1) grounding (RAG + tools),  
2) careful prompts & UX,  
3) decoding and validation constraints,  
4) targeted fine-tuning and alignment.  
In production, always combine LLMs with **verification and guardrails**, especially for critical decisions.

