# Neural Network Architectures - Interview Notes

---

## 2.1 ANN — Pros / Cons

**Artificial Neural Networks (ANNs)** are generic multi-layer perceptrons (MLPs) that map fixed-size input vectors to outputs.  
**Pros:** Flexible, can approximate complex non-linear functions, simple to implement, good for tabular data.  
**Cons:** Don’t scale well to high-dimensional structured data like images or sequences without architecture tweaks; can overfit and need careful tuning.  

---

## 2.2 CNN — Structure, Layers, Activation, Kernel Size

A **Convolutional Neural Network (CNN)** uses convolution layers to exploit spatial structure (e.g., images).  
Typical stack: `Conv → Activation (ReLU) → Pooling → (repeat) → Fully Connected → Output`.  
Kernels (filters) are small (e.g., 3×3, 5×5) and slide over the input to detect local patterns, sharing parameters across locations.  
Pooling layers (max/avg) downsample; ReLU (or variants) adds non-linearity and mitigates vanishing gradients.

---

## 2.3 CNN vs FCNN vs FFNN — Differences

- **FFNN / FCNN / MLP:** Fully connected layers where every neuron connects to all neurons in the previous layer; no explicit spatial structure.  
- **CNN:** Uses local receptive fields and weight sharing to exploit spatial locality (height/width), far fewer parameters for images.  
- FCNNs treat all features equally; CNNs assume nearby features are related and translation patterns matter.  
- CNN is preferred for images; FCNN/MLP is typical for tabular & simple structured data.

---

## 2.4 RNN Motivation

**Recurrent Neural Networks (RNNs)** were introduced to handle **sequences** where order matters (text, time series, audio).  
They maintain a **hidden state** that carries information from previous time steps, enabling modeling of temporal dependencies.  
Unlike feedforward nets, RNNs can, in principle, use arbitrarily long context.  
They are the precursor to LSTMs, GRUs, and eventually Transformers.

---

## 2.5 LSTM vs Vanilla RNN — What Problem Is Solved?

Vanilla RNNs struggle with **vanishing/exploding gradients**, making it hard to learn long-term dependencies.  
**LSTMs (Long Short-Term Memory)** introduce gates (input, forget, output) and a cell state to control what to keep, forget, or output.  
This architecture allows gradients to flow over longer time spans, reducing vanishing gradient issues.  
As a result, LSTMs capture longer-context patterns (e.g., long sentences, long time series) more effectively than simple RNNs.

---

## 2.6 Memory Networks

**Memory Networks** augment neural models with an explicit, addressable **memory component**.  
They separate computation into: write to memory, read from memory (using attention or addressing), and output reasoning.  
Useful for tasks requiring multi-step reasoning, long-term context, or explicit knowledge retrieval (e.g., QA over documents).  
They inspired later architectures like Neural Turing Machines and modern retrieval-augmented models.

---

## 2.7 Capsule Networks

**Capsule Networks (CapsNets)** group neurons into “capsules” that encode both **presence and pose** (orientation, position, etc.) of features.  
They use **dynamic routing** between capsules instead of max-pooling, aiming to preserve spatial hierarchies and relationships.  
Goal: recognize objects even when their parts are transformed or arranged differently.  
Promising conceptually, but less widely adopted in practice compared to CNNs/Transformers.

---

## 2.8 Autoencoders

An **Autoencoder** learns to reconstruct its input through a bottleneck latent layer.  
Encoder compresses input to a latent vector; decoder reconstructs from this latent space, trained via reconstruction loss.  
Uses: dimensionality reduction, denoising, anomaly detection, pretraining representations.  
It learns compact, task-agnostic features of the input data.

---

## 2.9 Variational Autoencoders (VAEs)

**VAEs** are probabilistic autoencoders that learn a **distribution** over latent variables rather than a single point.  
Encoder outputs mean and variance for a latent Gaussian; decoder samples from this latent distribution to reconstruct.  
The loss combines reconstruction error with a KL-divergence term to regularize the latent space toward a prior.  
VAEs are **generative**: sampling from the prior and decoding produces new, realistic-looking data.

---

## 2.10 Parameter Sharing in DL

Parameter sharing means reusing the **same weights** across multiple positions or steps in a model.  
CNNs share filter weights across spatial locations; RNNs share weights across time steps.  
This drastically reduces the number of parameters and encodes useful inductive biases (translation invariance, time invariance).  
It improves generalization and makes training feasible on large inputs.

---

## 2.11 Self-Supervised Learning — Examples

Self-supervised learning uses **pretext tasks** where labels are derived from the data itself.  
Examples:  
- NLP: masked language modeling (BERT), next-token prediction (GPT), sentence order prediction.  
- Vision: predicting rotated angle, contrastive learning (SimCLR), masked patch prediction (MAE, ViT).  
Models pre-trained this way can be fine-tuned on downstream tasks with much less labeled data.

---

## 2.12 Curriculum Training

Curriculum training feeds the model **easier examples first**, then gradually increases difficulty.  
Inspired by human learning: solve simple tasks, then more complex ones once basics are mastered.  
It can stabilize training, speed up convergence, and sometimes improve generalization or robustness.  
A variant is **self-paced learning**, where the model itself decides which examples to focus on next.

```

