# Diffusion Models - Interview Notes

---

## 1. DDPM vs DDIM

**DDPM (Denoising Diffusion Probabilistic Models):**  
- Defines a **Markov chain**: data → gradually add Gaussian noise over T small steps.  
- Learns a reverse-time denoising process to approximate the posterior of each step.  
- Sampling: start from pure noise and run **many reverse steps** → high quality but **slow**.  
- Intuition: “Walk into noise in tiny steps, then learn how to walk back.”

**DDIM (Denoising Diffusion Implicit Models):**  
- Uses a **non-Markovian**, often deterministic, reverse process derived from DDPM.  
- Allows **fewer sampling steps** (e.g., 20–50 instead of hundreds or thousands) while keeping similar quality.  
- Supports things like better **latent control** and sometimes deterministic sampling (same seed → same result).  
- Intuition: “Skip some steps in a controlled way but still end up at good samples.”

---

## 2. Forward Diffusion & Denoising Process

- **Forward process (q):**  
  - Inject Gaussian noise step by step:  
    \[
    x_t = \sqrt{\alpha_t} x_0 + \sqrt{1 - \alpha_t} \epsilon
    \]
    where \(\epsilon \sim \mathcal{N}(0, I)\).  
  - After many steps, \(x_T\) ≈ pure noise; this process is **fixed and known** (no training).
- **Reverse (denoising) process (p\_\theta):**  
  - Train a neural net to predict either:
    - The **noise** added at step t, or  
    - The **clean data** \(x_0\), or a hybrid (v-parameterization).  
  - At inference: start from Gaussian noise and **iteratively denoise** from \(x_T \rightarrow x_0\).  
- Mental model:  
  - Forward: “Corrupt” the image gradually.  
  - Reverse: Learn an intelligent “undo” button for each noise level.

---

## 3. Loss Functions in Diffusion Models

- Common training objective: **predict the noise** \(\epsilon\) that was added at each step.  
- Standard loss:  
  \[
  \mathcal{L} = \mathbb{E}_{x_0, t, \epsilon} \left[ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 \right]
  \]
  → simple **MSE between true noise and predicted noise**.  
- Variants:
  - **x₀-prediction:** model predicts clean image directly.  
  - **v-prediction:** a reparameterized target that stabilizes training and sampling.  
- Big picture:  
  - Different parameterizations change optimization and sample quality, but the core idea is **“match the true reverse diffusion dynamics”**.

---

## 4. Score Matching Loss (Score-based Diffusion)

- **Score function:**  
  \[
  s(x) = \nabla_x \log p(x)
  \]
  points toward high-density regions of the data distribution.  
- **Score-based diffusion** trains a model to approximate this score at different noise levels.  
- **Denoising Score Matching:**  
  - Add noise to data and train the model to predict how to move back toward higher density.  
  - Loss encourages model’s score to match the true data score.  
- Sampling:
  - Use a **stochastic differential equation (SDE)** or ODE that uses the learned score to gradually transform noise → data.  
- Intuition: “Learn the direction to walk from any noisy point back into the data manifold.”

---

## 5. Why “Stable” in Stable Diffusion?

- “Stable” is partly brand (**Stability AI**) and partly about **stable, efficient generation**.  
- Key design choice: operate in a **latent space**, not pixel space:
  - An encoder compresses images to a smaller latent representation.  
  - Diffusion runs in this latent space → less memory + faster training/inference.  
  - A decoder reconstructs images from latents.  
- Benefits:
  - High-resolution images with **lower compute + memory**.  
  - Training becomes more numerically stable and scalable.  
- Mental hook: “Do diffusion on a **smaller canvas** (latent), then upscale to pixels.”

---

## 6. High-Level Functioning vs Other Generative Models

### Diffusion Models
- Start from pure noise and **iteratively denoise** across many steps.  
- Very **stable training** (no adversarial game), excellent **mode coverage** and **image quality**.  
- Downsides: sampling can be **slow** (many steps), though DDIM/other accelerations help.

### GANs (Generative Adversarial Networks)
- Train **Generator vs Discriminator** in an adversarial game.  
- Pros:
  - **One-shot sampling** → very fast generation.  
  - Very sharp, realistic images (when training works well).  
- Cons:
  - Training is **unstable**, prone to **mode collapse** (missing parts of data distribution).  
  - Sensitive to architecture and hyperparameters.

### VAEs (Variational Autoencoders)
- Learn a **probabilistic encoder–decoder** with KL-regularized latent space.  
- Pros:
  - Stable training, nice **latent structure** (good for interpolation, editing).  
- Cons:
  - Reconstructions and samples often look **blurry** compared to GANs/diffusion.  
- Mental model comparison:
  - **VAE:** “Compress and sample from a smooth latent world.”  
  - **GAN:** “Battle between a faker and a detective.”  
  - **Diffusion:** “Step-by-step cleaning of noise into data.”

---

## 7. Key Interview Hooks (Short Answers)

- **Why are diffusion models popular now?**  
  Stable training, strong diversity, and **state-of-the-art image quality** vs GANs/VAEs.

- **Why is sampling slow, and how do we speed it up?**  
  Many reverse steps; use **DDIM**, fewer steps, or specialized samplers (e.g., DPM-Solver).

- **What’s the core training idea?**  
  Corrupt data with known noise, then train a network to **reverse the corruption** (predict noise / score).

- **Why latent diffusion (e.g., Stable Diffusion)?**  
  Run diffusion in **compressed latent space** → faster, cheaper, more scalable for high-res images.
