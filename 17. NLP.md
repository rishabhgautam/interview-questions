# NLP Basics - Interview Notes

---

## 1. Word Embeddings

Word embeddings map words to **dense vectors** so similar words have nearby vectors.  
They capture semantic relations (king ~ queen, Paris ~ London) via co-occurrence patterns.  
Examples: Word2Vec, GloVe, FastText, contextual embeddings from BERT.  

---

## 2. Sentence Encoding

Sentence encoding turns a full sentence (or document) into a **single vector**.  
This vector can be used for similarity search, retrieval, clustering, or classification.  
Built using averaged word embeddings, RNNs/CNNs, or transformer-based models (e.g., Sentence-BERT).  

---

## 3. POS Tagging (Part-of-Speech Tagging)

POS tagging assigns each token a **grammatical label** (NOUN, VERB, ADJ, etc.).  
It helps parse sentence structure and supports downstream tasks like NER and parsing.  
Modern POS taggers use sequence models (BiLSTM-CRF, Transformers) for context-aware tagging.  

---

## 4. NER (Named Entity Recognition)

NER identifies and classifies **named entities** (PERSON, ORG, LOC, DATE, etc.) in text.  
Used in information extraction, search, knowledge graphs, and analytics.  
Typically implemented with contextual embeddings plus sequence labeling (CRF/Transformer heads).  

---

## 5. Text Classification

Text classification assigns labels like **spam/not spam, sentiment, topic, intent** to text.  
Traditional: bag-of-words / TF-IDF with logistic regression or SVM.  
Modern: fine-tuned transformer encoders (e.g., BERT) on labeled data.  

---

## 6. Text Similarity

Text similarity measures **how close in meaning** two texts are.  
Simple: cosine similarity over TF-IDF or average embeddings.  
Strong: sentence-level embeddings (Sentence-BERT) or cross-encoder models for pair scoring.  

---

## 7. Text Clustering

Text clustering groups documents into clusters **without labels**.  
Pipeline: embed texts → cluster using K-means, hierarchical clustering, or density-based methods.  
Useful for topic discovery, FAQ grouping, and exploratory analysis.  

---

## 8. Stemming vs Lemmatization

**Stemming:** heuristic chopping of word endings (run, running, runs → run-ish forms); fast but rough.  
**Lemmatization:** uses vocabulary and grammar to get the **true base form** (better → good, running → run).  
Stemming is noisy but fast; lemmatization is slower but more accurate and interpretable.  

---

# Transformer & Attention in NLP

---

## 1. What Is the Attention Mechanism in NLP?

Attention lets a model **focus on the most relevant tokens** when processing a sequence.  
Each token computes weights over other tokens and takes a weighted sum of their representations.  
This allows modeling **long-range dependencies** and context-sensitive meaning (e.g., pronouns, word sense).  

---

## 2. What Are Transformer Models in NLP?

Transformers are sequence models built from **stacked self-attention + feedforward blocks**, no recurrence.  
They process all tokens in parallel and use attention to mix information across positions.  
Variants: encoder-only (BERT), decoder-only (GPT), encoder–decoder (T5, original Transformer).  
They power most modern NLP systems: translation, QA, summarization, chat, and more.  

---

# Classical NLP Tasks

---

## 1. Summarization

Summarization generates a shorter version of a text while preserving key information.  
Extractive: select important sentences or phrases.  
Abstractive: generate new sentences capturing the gist (usually with seq2seq/Transformers).  

---

## 2. Dependency Parsing

Dependency parsing finds **who depends on whom** in a sentence (head–dependent relations).  
It represents syntax as a graph: e.g., “ate” is the head, “she” is subject, “apple” is object.  
Helps with information extraction, relation extraction, and deeper linguistic analysis.  

---

## 3. Coreference Resolution

Coreference resolution finds all **expressions that refer to the same entity**.  
Example: “Rishabh went home. He was tired.” → “Rishabh” and “He” corefer.  
Used in QA, summarization, and any task needing consistent entity tracking across text.  

---

## 4. Dialogue Systems

Dialogue systems (chatbots, assistants) manage **multi-turn conversations** with users.  
They combine intent detection, state tracking, response generation/retrieval, and policies.  
Modern systems often use LLMs plus tools / memory / retrieval for grounded answers.  

---

## 5. Text Generation

Text generation creates new text given a prompt (e.g., translation, story writing, code generation).  
Implemented via language models that predict the next token repeatedly.  
Quality depends on model training, decoding strategy (greedy, beam, sampling), and constraints.  

---

## 6. Named Entity Disambiguation (NED)

NED links detected entities in text to **canonical entries in a knowledge base** (e.g., “Apple” → company vs fruit).  
It goes beyond NER by resolving ambiguity between entities with the same surface form.  
Essential for building knowledge graphs, search, and structured reasoning over text.  

---

# Other — Common LLM/NLP Questions

---

## 1. Is BERT a Text Generation Model?

BERT is primarily an **encoder-only** model trained with masked language modeling.  
It’s designed for understanding tasks (classification, NER, QA) rather than free-form generation.  
It can generate text in some constrained ways, but GPT-style models are better suited for generation.  

---

## 2. Weight Tying in Language Models

Weight tying means **reusing the same embedding matrix** for input embeddings and output softmax projection.  
This reduces parameters and enforces consistency between how tokens are read and predicted.  
Common in LMs because input and output vocab spaces are identical.  

---

## 3. Special Tokens Meaning

Special tokens are reserved IDs with structural roles, such as:  
- `[CLS]` / `<cls>`: classification token, summary representation.  
- `[SEP]`: separator between segments (sentence A/B).  
- `<bos>` / `<eos>`: begin/end of sequence.  
- `<pad>`: padding for batching.  
They help models understand **boundaries, roles, and structure** in sequences.  

