# ğŸ“˜ NLP Basics - Interview Notes

---

##  1. Word Embeddings

- Maps words â†’ **dense continuous vectors**  
- Similar context â†’ similar vectors (distributional hypothesis)  
- Captures semantic + syntactic relations  
  - *king â€“ man + woman â‰ˆ queen*  
- **Static embeddings** (one vector per word):  
  - Word2Vec (CBOW, Skip-Gram), GloVe, FastText (subword awareness)
- **Contextual embeddings** (different vectors per context):  
  - BERT, GPT, ELMo

ğŸ§  *Memory Hook:* **Meaning becomes geometry in vector space.**

---

##  2. Sentence Encoding

- Convert a sentence/doc â†’ **single semantic vector**  
- Used for: retrieval, ranking, clustering, paraphrase mining  
- Methods:
  - Avg word embeddings (simple)
  - RNN/CNN encoders
  - **Transformer encoders** (e.g., Sentence-BERT, USE)

ğŸ§  *Memory Hook:* One vector captures the **whole meaning**.

---

##  3. POS Tagging (Part-of-Speech)

- Assigns word roles: **NOUN / VERB / ADJ / ADV** etc.  
- Helps grammar-aware tasks and downstream reasoning  
- Sequence models with context:
  - BiLSTM-CRF  
  - Transformers (e.g., BERT tagging head)

ğŸ§  *Memory Hook:* **Tag the role** each word plays.

---

##  4. NER â€” Named Entity Recognition

- Detects entities: PERSON, ORG, LOC, PRODUCT, DATE, etc.  
- Sequence labeling using BIO/BILOU tagging  
- Key uses: info extraction, analytics, knowledge graph population  
- Transformer-based NER is SOTA today

ğŸ§  *Memory Hook:* **Highlight** the important nouns in text.

---

##  5. Text Classification

- Map text â†’ labels  
- Examples: spam, sentiment, topic, intent  
- Traditional: TF-IDF + Logistic Regression / SVM  
- Modern: Fine-tune BERT / RoBERTa / DistilBERT

ğŸ§  *Memory Hook:* **Label** the text.

---

##  6. Text Similarity

- Measure semantic closeness between two texts  
- Simple: cosine(TF-IDF), Jaccard  
- Stronger: Sentence-BERT embeddings  
- Best accuracy: **Cross-encoders** (but slower)

ğŸ§  *Memory Hook:* **Close vectors â‡’ close meaning**.

---

##  7. Text Clustering

- Group unlabeled documents into topics  
- Pipeline: Embeddings â†’ K-Means / HDBSCAN / Spectral  
- Useful for:
  - FAQ deduplication  
  - Topic discovery  
  - Customer feedback insights

ğŸ§  *Memory Hook:* Similar meanings **self-organize**.

---

##  8. Stemming vs Lemmatization

| Property | Stemming | Lemmatization |
|---------|-----------|---------------|
| Method | Rule-based suffix chop | Dictionary + grammar |
| Output | Sometimes invalid | True root form |
| Speed | Fast | Slower |
| Example | â€œbetterâ€ â†’ â€œbettâ€ | â€œbetterâ€ â†’ â€œgoodâ€ |

ğŸ§  *Memory Hook:*  
Stemming = **cut endings**  
Lemmatization = **true base form**

---

# âš¡ Transformers & Attention in NLP

---

##  1. What is Attention?

- Allows each token to **weigh** importance of other tokens  
- Outputs a **weighted sum** of value vectors  
- Handles long-range context extremely well  
- Key for pronoun resolution, entity linking, ambiguity handling

ğŸ§  *Memory Hook:* â€œ**Pay attention** to the right words.â€

---

##  2. What are Transformer Models?

- Architecture: **Self-Attention â†’ Feedforward â†’ Residual â†’ LayerNorm**  
- No recurrence â†’ **parallel compute** & long context  
- Types:
  - Encoder-only â†’ **BERT** (understanding)
  - Decoder-only â†’ **GPT** (generation)
  - Encoder-decoder â†’ **T5 / PaLM-E** (seq2seq)

ğŸ§  *Memory Hook:* **Attention replaces recurrence**.

---

# ğŸ§© Key Classical NLP Tasks

---

##  1. Summarization

- **Extractive**: pick sentences  
- **Abstractive**: rewrite meaning using generation  
- Metrics: ROUGE-N, ROUGE-L, BERTScore, human eval

ğŸ§  *Memory Hook:* Shorter **with same meaning**.

---

##  2. Dependency Parsing

- Finds **grammatical structure**: headâ†’dependent  
- Example: â€œShe ate pizzaâ€
  - ate=head, she=nsubj, pizza=dobj  
- Outputs tree/graph used for relation reasoning

ğŸ§  *Memory Hook:* Who does **what** to **whom**?

---

## 3. Coreference Resolution

- Link mentions â†’ same entity  
  *"Rishabh went home. He slept."*  
- Crucial for multi-sentence reasoning & QA

ğŸ§  *Memory Hook:* **Track identities** across text.

---

##  4. Dialogue Systems

- Multi-turn agent pipeline:
  1) NLU â†’ 2) State Tracking â†’ 3) Policy â†’ 4) NLG  
- LLM-based agents + RAG dominate current systems  
- Metrics: success rate, turn efficiency

ğŸ§  *Memory Hook:* **Understand â†’ Decide â†’ Talk**.

---

##  5. Text Generation

- Autoregressive **next-token prediction**  
- Controlled via decoding:
  - Greedy, Beam Search  
  - Top-k / Top-p / Temperature  
- Issues: hallucination, repetition, safety risks

ğŸ§  *Memory Hook:* **Predict one token at a time**.

---

##  6. Named Entity Disambiguation (NED)

- Map entity mention â†’ **unique KB entry**  
  - â€œAppleâ€ â†’ ğŸ vs ï£¿  
- Often uses retrieval + contextual similarity

ğŸ§  *Memory Hook:* â€œ**Which Apple?**â€

---

# ğŸ¯ Other Popular Interview Qâ€™s

---

##  1. Is BERT a Text Generation Model?

- âŒ No â€” encoder-only, **understanding-focused**
- Can fill in masked tokens but not true open-generation like GPT

ğŸ§  *Memory Hook:* BERT **reads**, GPT **writes**.

---

## 2. Weight Tying in LLMs

- Share embedding matrix for:
  - Input token embeddings  
  - Output softmax projection  
- Saves parameters and improves consistency

ğŸ§  *Memory Hook:* â€œ**Same book** for reading and writing.â€

---

## 3. Special Tokens

| Token | Purpose |
|------|---------|
| `<cls>` | Classification pooling |
| `<sep>` | Separate sequences |
| `<bos>` `<eos>` | Sequence boundaries |
| `<pad>` | Batch alignment |
| `<mask>` | Masked LM training |

ğŸ§  *Memory Hook:* Tokens that **organize structure**, not language.

---

## ğŸ’¡ Final Takeaway

> NLP = **Represent â†’ Understand â†’ Reason â†’ Generate**  
> Embeddings + Transformers = **modern NLP foundation**
