# NLP Basics — Interview Notes

---

## 1. Word Embeddings

Word embeddings map words to **dense vectors** so that similar words have similar representations.  
They capture semantic relationships like *king – man + woman ≈ queen*.  
Examples: Word2Vec, GloVe, FastText, and contextual embeddings from models like BERT.  
They are the foundation for most modern NLP pipelines.

---

## 2. Sentence Encoding

Sentence encoding maps an entire sentence (or paragraph) to a **single vector**.  
Useful for tasks like similarity search, retrieval, clustering, and classification.  
Can be built using averaged word embeddings, RNNs/CNNs, or transformer-based models (e.g., Sentence-BERT).  
Good sentence encoders place semantically similar sentences close in vector space.

---

## 3. POS Tagging (Part-of-Speech Tagging)

POS tagging assigns each word a **grammatical category** (noun, verb, adjective, etc.).  
It helps downstream tasks like parsing, NER, and information extraction.  
Modern systems use sequence models (BiLSTM-CRF, Transformers) to capture context.  
Same word can get different tags depending on context (“book a ticket” vs “read a book”).

---

## 4. NER (Named Entity Recognition)

NER identifies and labels **named entities** in text (e.g., PERSON, ORG, LOC, DATE).  
It’s key for information extraction, search, and knowledge graph construction.  
Models often use contextual embeddings + sequence tagging (CRF, Transformers).  
Challenges include ambiguous names, nested entities, and domain-specific terms.

---

## 5. Text Classification

Text classification assigns labels to documents or sentences (spam/ham, sentiment, topic, intent).  
Traditional approach: bag-of-words/TF-IDF + linear models (logistic regression, SVM).  
Modern approach: transformer encoders (e.g., BERT) fine-tuned for classification.  
Critical to choose the right labels, handle class imbalance, and evaluate with appropriate metrics.

---

## 6. Text Similarity

Text similarity measures how **semantically close** two pieces of text are.  
Basic methods: cosine similarity over TF-IDF or average embeddings.  
More advanced: sentence embeddings from transformers, cross-encoders that jointly encode both texts.  
Used in search, deduplication, recommendation, and clustering.

---

## 7. Text Clustering

Text clustering groups similar documents **without labels**.  
Pipeline: embed text (TF-IDF or sentence embeddings) → run clustering (K-means, hierarchical, DBSCAN).  
Useful for topic discovery, FAQ grouping, and exploratory analysis.  
Quality depends heavily on embedding quality and chosen distance metric.

---

## 8. Stemming vs Lemmatization

**Stemming:** crude rule-based stripping of suffixes (e.g., “running” → “run”, “runner” → “runner”).  
**Lemmatization:** uses vocabulary + morphology to return the **base form (lemma)** (e.g., “better” → “good”).  
Stemming is faster but rough; lemmatization is slower but more linguistically accurate.  
Choice depends on task: stemming for quick/rough search, lemmatization for analytic/linguistic tasks.


