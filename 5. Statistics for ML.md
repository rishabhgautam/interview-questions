```markdown
# Statistics for ML - Interview Notes

---

## 1. Correlation & Correlation Coefficient

Correlation measures the strength and direction of a linear relationship between two variables.  
The correlation coefficient lies between -1 and +1, where +1 is perfect positive, -1 is perfect negative, and 0 means no linear relationship.  
It is scale-invariant, so changing units (e.g., cm to m) does not change correlation.  
In ML, it’s often used for quick sanity checks and feature selection (but only captures linear relationships).

---

## 2. Pearson vs Spearman Correlation

**Pearson** measures linear correlation between two continuous variables; it’s sensitive to outliers and assumes a roughly linear relationship.  
**Spearman** is a rank-based correlation; it measures monotonic relationships (increasing or decreasing, not necessarily linear).  
Spearman is more robust to outliers and non-linear but monotonic trends.  
Use Pearson for roughly linear relationships; use Spearman when data is ordinal, skewed, or has clear rank structure.

---

## 3. Central Limit Theorem (CLT)

CLT says: the distribution of the **sample mean** tends to be approximately normal as sample size grows, regardless of the original distribution (under mild conditions).  
This is why we can use normal-based confidence intervals and hypothesis tests even when data is not perfectly normal.  
In practice, with sample size around 30–50+, the sample mean starts behaving “normal enough” for many problems.  
Used heavily in A/B testing, confidence intervals, and error bars.

---

## 4. Hypothesis Testing & p-value (Layman’s Terms)

Hypothesis testing starts with a **null hypothesis** (e.g., “no difference between groups”).  
You compute a test statistic and a **p-value**, which is the probability of seeing data as extreme as yours if the null were actually true.  
A small p-value (commonly below 0.05) suggests the result is unlikely under the null, so you may reject it.  
It does *not* mean “there is a 95% chance the hypothesis is true” — it’s always conditioned on the null being assumed true.

---

## 5. Bernoulli vs Binomial Distribution

A **Bernoulli** distribution models a single trial with two outcomes (e.g., success/failure) with probability p of success.  
A **Binomial** distribution models the number of successes in n independent Bernoulli trials with the same p.  
So Bernoulli is “one flip of a biased coin”, binomial is “how many heads in n flips”.  
In ML, many binary events (click/no-click, churn/no-churn) are modeled at this level.

---

## 6. Probability Puzzle — Dice: 3 Rolls, 2 Consecutive 3s

We roll a fair die three times and want the probability of getting two consecutive 3s.  
The pairs (1,2) or (2,3) must both be 3s; overlapping case is when all three are 3.  
Using inclusion–exclusion, the probability works out to 11/216 (about 5.1%).  
Useful for practicing pattern probability and overlapping-event reasoning.

---

## 7. Probability Puzzle — HH vs TH Game

You toss a fair coin until either **HH** or **TH** appears; you win if HH appears first, your friend wins if TH appears first.  
Conditioning on the first toss, it turns out that once a T appears, TH will eventually show up with probability 1 before HH.  
If the first toss is H, you still only win if the very next toss is also H.  
Overall, your probability of winning is 1/4, and your friend’s is 3/4.

---

## 8. Probability Puzzle — 30 People & the Birthday Paradox

The birthday paradox asks: in a group of 30 people, what is the probability that at least two share a birthday?  
Instead of computing the match probability directly, we compute the probability that all birthdays are different and subtract from 1.  
For 30 people, the probability of at least one shared birthday is roughly about 70%.  
Key takeaway: collisions happen much more often than intuition suggests.

---

## 9. Left-Skewed Distribution: Mean, Median, Mode

A left-skewed (negatively skewed) distribution has a long tail toward **lower** values.  
The mean is pulled toward the tail, so typically: **mean < median < mode**.  
If the median is 60, you would expect the mean to be **less than 60** and the mode **greater than 60**.  
Important when interpreting summary statistics, especially with skewed financial or time-to-event data.

---

## 10. Zipf’s Law

Zipf’s law describes distributions where the frequency of an item is inversely proportional to its rank.  
In language, the most frequent word appears about twice as often as the second most frequent, three times as often as the third, etc.  
It is a type of power-law / long-tail behavior seen in word frequencies, city sizes, website hits, etc.  
For ML, it explains why rare events/classes are very common in real-world data and why long-tail handling is critical.

---

## 11. Cohen’s Kappa vs Krippendorff’s Alpha

Both measure **inter-rater agreement** beyond chance.  
**Cohen’s Kappa** is mainly for two raters and works best with nominal categories and complete data.  
**Krippendorff’s Alpha** is more general: supports multiple raters, various data types (nominal, ordinal, interval), and missing data.  
Use Kappa for simple two-rater problems; use Alpha when you have more raters, missing labels, or non-nominal scales.

```

