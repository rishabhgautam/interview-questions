# Statistics for ML - Interview Notes

---

## 1. Correlation & Correlation Coefficient

- **Definition:** Measures **strength and direction of a linear relationship** between two variables.  
- **Range:** -1 (perfect negative), 0 (no linear relationship), +1 (perfect positive).  
- **Properties:**  
  - Scale-invariant: unit changes don’t affect correlation.  
  - Symmetric: Corr(X,Y) = Corr(Y,X).  
- **ML Use Cases:**  
  - Quick feature sanity check.  
  - Feature selection: remove highly correlated features to reduce multicollinearity in linear models.  
  - Caution: correlation only captures linear relationships; non-linear relationships may exist even if correlation ≈ 0.

---

## 2. Pearson vs Spearman Correlation

- **Pearson:** Measures linear correlation between two continuous variables.  
  - Sensitive to outliers.  
  - Assumes roughly linear relationships.  
- **Spearman:** Rank-based, measures monotonic relationships.  
  - Robust to outliers and skewed distributions.  
  - Works with ordinal data.  
- **Practical Tip:**  
  - Pearson → linear regression, PCA, linear feature selection.  
  - Spearman → non-linear but monotonic patterns, robust ranking, feature importance ordering.

---

## 3. Central Limit Theorem (CLT)

- **Statement:** The **distribution of sample means** tends to normal as sample size grows (n ≥ 30 is often sufficient), even if underlying data is non-normal.  
- **Implications for ML:**  
  - Confidence intervals for means.  
  - Hypothesis testing with t-tests or z-tests.  
  - Bootstrapping approximates CLT for complex features.  
- **Example:** Estimating average customer spend with sample size 50 yields roughly normal mean distribution regardless of skewed spend data.

---

## 4. Hypothesis Testing & p-value (Layman’s Terms)

- **Null Hypothesis (H0):** “No effect” or “no difference.”  
- **Alternative Hypothesis (H1):** What you want to test.  
- **p-value:** Probability of observing data at least as extreme as the sample if H0 is true.  
- **Common Misconceptions:**  
  - p < 0.05 ≠ 95% chance H1 is true.  
  - p > 0.05 ≠ proof of no effect; it may be underpowered.  
- **ML Relevance:**  
  - A/B testing.  
  - Feature significance in linear/logistic regression.  
  - Detecting covariate imbalance in experimental design.

---

## 5. Bernoulli vs Binomial Distribution

- **Bernoulli:** Single trial → success (1) or failure (0), probability p of success.  
- **Binomial:** Number of successes in n independent Bernoulli trials with probability p.  
- **Example:**  
  - Bernoulli: Did a customer churn this month? (Yes/No)  
  - Binomial: Number of churns in a sample of 100 customers.  
- **ML Relevance:**  
  - Logistic regression models Bernoulli outcomes.  
  - Binomial likelihood used in evaluation of classification models.

---

## 6. Probability Puzzle — Dice: 3 Rolls, 2 Consecutive 3s

- **Problem:** Probability of exactly two consecutive 3s in 3 dice rolls.  
- **Logic:** Enumerate sequences: (3,3,1/2/4/5/6), (1/2/4/5/6,3,3), minus overlapping case (3,3,3).  
- **Result:** 11/216 ≈ 5.1%.  
- **Takeaway:** Useful to understand **overlapping events** and inclusion–exclusion principle.

---

## 7. Probability Puzzle — HH vs TH Coin Game

- **Problem:** Toss coin until either HH or TH appears. Probability of HH first?  
- **Solution Insight:** Condition on first toss:  
  - First toss H → next H needed → probability 1/4.  
  - First toss T → TH eventually appears with probability 1.  
- **Takeaway:** Conditional probability and sequence patterns often counter intuitive; useful for **Markov chain reasoning**.

---

## 8. Probability Puzzle — Birthday Paradox

- **Problem:** Probability at least 2 people share a birthday in group of 30.  
- **Method:** Compute probability all birthdays are different → subtract from 1.  
- **Result:** ≈ 70%.  
- **ML Relevance:** Understanding collisions in hash functions, class overlap, and **rare-event probabilities**.

---

## 9. Left-Skewed Distribution: Mean, Median, Mode

- **Left-skewed (negative skew):** Tail toward smaller values.  
- **Relationship:** Mean < Median < Mode.  
- **ML Relevance:**  
  - Feature transformations: log or sqrt to reduce skew.  
  - Choosing median as robust statistic over mean for outlier-prone distributions.

---

## 10. Zipf’s Law

- **Definition:** Frequency of items ~ 1/rank.  
- **Examples:** Word frequency, city sizes, website hits.  
- **ML Implications:**  
  - Long-tail distributions are common.  
  - Need strategies for rare classes: oversampling, hierarchical modeling, embeddings.  

---

## 11. Cohen’s Kappa vs Krippendorff’s Alpha

- **Cohen’s Kappa:** Measures agreement between 2 raters; accounts for chance.  
- **Krippendorff’s Alpha:** Generalizes to multiple raters, data types, missing labels.  
- **ML Use Cases:**  
  - Label consistency evaluation.  
  - Preprocessing for noisy or crowdsourced datasets.  

---

## 12. Additional Statistical Concepts for ML

- **Skewness & Kurtosis:**  
  - Skew: asymmetry of distribution.  
  - Kurtosis: “tailedness”; high kurtosis → extreme outliers more likely.  

- **Law of Large Numbers:**  
  - Sample mean converges to population mean as sample size grows.  
  - Important for A/B testing and model evaluation metrics.  

- **Bayes Theorem:**  
  - Posterior ∝ Likelihood × Prior.  
  - Core of Naive Bayes classifier, probabilistic modeling, and decision-making under uncertainty.  

- **Confusion Matrix Metrics:**  
  - Precision, Recall, F1-score, Accuracy, ROC-AUC.  
  - Interpret metrics depending on **class imbalance**.  

- **Entropy & Information Gain:**  
  - Measure uncertainty in a variable.  
  - Used in feature selection for trees (Decision Trees, Random Forests, Gradient Boosted Trees).  

- **Chi-Square Test for Independence:**  
  - Check if categorical features are dependent or independent.  
  - Useful for feature selection and hypothesis validation in categorical data.

---

**Summary Mental Models:**  
- Correlation = linear association; Spearman = monotonic trends.  
- CLT explains why sample averages behave normally → confidence intervals.  
- p-value = “likelihood of data under null,” not probability of truth.  
- Long-tail & Zipf → rare events frequent in real data → handle carefully.  
- Sequence & pattern probabilities (dice, coin games) = conditioning + overlapping events.  
- Label agreement metrics = ensure data quality for ML.

