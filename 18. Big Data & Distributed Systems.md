# ğŸš€ Big Data & Distributed Systems â€” Interview Notes

---

## ğŸ§± 1ï¸âƒ£ Big Data Concepts

---

### ğŸ”¹ A) Small-File vs Big-File Problem

| Small Files â†’ Bad | Why |
|---|---|
| Too many file handles | NameNode / metadata overhead |
| Too many tasks | Scheduler thrash, high latency |
| Poor compression ratios | Compression works better on large datasets |
| Poor scan efficiency | Many seeks + open/close ops |

**Best Practices**
- Prefer **128â€“1024 MB** file sizes
- Use **repartition / coalesce** to compact outputs  
- Store in **Parquet** instead of JSON/CSV
- Periodic compaction of streaming outputs (Kafka â†’ Spark)

ğŸ§  *Rule of Thumb:* If you have **millions of files**, performance is **dying**.

---

### ğŸ”¹ B) Why Parquet Rocks for Analytics

- **Columnar format** â†’ read only needed fields â†’ low I/O  
- **Efficient encoding + compression**
  - Dictionary, run-length, delta encoding
- **Predicate + projection pushdown**  
  (filters and column selection done at storage)
- **Schema evolution** supported: `add/drop columns`
- **Vectorized execution** in Spark/Presto engines

Performance Tip:  
> `parquet.enable.dictionary=true` improves compression on categorical fields.

---

### ğŸ”¹ C) Spark Performance Pitfalls & Fixes

| Pitfall | Fix |
|---|---|
| Too many tiny partitions | `df.coalesce(n)` or `df.repartition(n)` |
| Data skew on joins | Salting, skew hints, broadcast join |
| Excess shuffles | Cache + reuse; avoid unnecessary wide ops |
| Blind caching | Cache only reused data; monitor memory |
| CSV/JSON ingestion | Prefer Parquet/ORC + pruning |

Skew Symptoms:
- Few tasks run **much slower**
- Executors idle waiting for stragglers

Skew Mitigations:
- Salt hot keys  
- Pre-aggregate before join  
- Use **AQE** (Adaptive Query Execution) in Spark 3+

---

## ğŸŒ 2ï¸âƒ£ Distributed Systems Essentials

---

### ğŸ”¹ A) Redis â€” Where It Fits

| Type | Use Cases |
|---|---|
| In-memory key-value store | Ultra-low-latency reads/writes |
| Cache | API responses, auth tokens, features |
| Data structures | Leaderboards (sorted sets), counters |
| Pub/Sub | Realtime notifications |
| Distributed Locks | Redlock algorithm (but use carefully) |

Persistence:
- **RDB** snapshots (fast recovery)
- **AOF** append-only logs (stronger durability)

ğŸ§  If response must be **<1 ms**, Redis is your friend.

---

### ğŸ”¹ B) Kafka Pub-Sub â€” Mental Model

Kafka is a **distributed commit log**:

```

Topic
â””â”€â”€ Partitions (ordered)
â””â”€â”€ Records (immutable)

```

Core Properties:
- High throughput (MBsâ€“GBs/sec)
- Message retention (not just queueing)
- Consumer-group scaling  
  â†’ each partition consumed by **only 1** consumer in a group
- Replayable by offset control

Delivery semantics:
- **At least once** default
- **Exactly once** with idempotent writes + transactions

ğŸ§  Partitioning = **scaling**  
ğŸ§  Offsets = **reliability + replay**

---

### ğŸ”¹ C) How Consistent Hashing Works (Simple)

1ï¸âƒ£ Hash nodes onto a ring  
2ï¸âƒ£ Hash keys onto the ring  
3ï¸âƒ£ Key owned by **first node clockwise**

When a node joins/leaves â†’  
only small region around it remaps â†’ minimal movement

Add **virtual nodes**:
- Better load balancing
- Smooth scaling

ASCII Visual:
```

(hash space ring)
N3
/     
N2       N4
\     /
N1

```

ğŸ§  Works great for **caches** + **sharded NoSQL**.

---

### ğŸ”¹ D) Load Balancing Algorithms

| Method | Strength | Weakness |
|---|---|---|
| Round Robin | Simple | Doesnâ€™t consider load |
| Weighted RR | Simple, capacity-aware | Needs manual weights |
| Least Connections | Real-time balancing | Needs live metrics |
| IP Hash | Session stickiness | Uneven distribution |
| Consistent Hash | Stable remapping | More complex to implement |
| Random w/ 2 choices | Great load distribution | Requires load metadata |

Industry Tip:
> â€œRandom with two choicesâ€ is surprisingly powerful and scalable.

---

### ğŸ”¹ E) NoSQL Indexing (High-level)

Common Index Structures:
- **Primary index** (hash-based or sorted)
- **Secondary index**
  - Local index (per shard)
  - Global index (across shards)
- **Inverted index** â€” text search
- **B-Trees** â€” ordered queries
- **LSM Trees** â€” write-optimized (Cassandra, Bigtable)

Trade-offs:
- **Local index** = fast write, slower cross-shard reads  
- **Global index** = fast reads, expensive write amplification

ğŸ§  Always design **indexes based on queries**, not vice-versa.

---

### ğŸ”¹ F) Consistent Hashing vs Normal Hashing

| Aspect | Normal Hashing | Consistent Hashing |
|---|---|---|
| Key placement | `hash(key) % N` | Ring traversal |
| Node scaling | Remap almost everything | Move **minimal keys** |
| Load balance | Usually poor | Good with virtual nodes |
| Common use | Static clusters | Dynamic growing clusters |

---

## ğŸ§ª 3ï¸âƒ£ Interview Questions â€” Practice

### Spark & Data Engineering
- What causes **data skew**? Mitigation strategies?
- Difference: `coalesce()` vs `repartition()`?
- How AQE improves performance in Spark 3?
- Why Parquet for analytics?
- How to handle **small files** in streaming?

### Distributed Systems
- At-least-once vs exactly-once in Kafka?
- How **leader election** works in distributed clusters?
- How does Redis ensure high availability?
- CAP theorem: where do Redis, Cassandra, Kafka sit?
- How consistent hashing avoids mass data movement?

### Caching & Scaling
- Cache invalidation strategies?
- CDN vs load balancer?
- Circuit breakers & backpressure patterns?
- Thundering herd prevention?

---

## ğŸ§  Handy Rules of Thumb

- Prefer **columnar** + **large files** for BI
- Partitioning keys must reflect **query filters**
- Balance **# of partitions = 2â€“4 Ã— # of cores**
- Distributed locks are always **tricky** â†’ avoid if possible
- Scaling is easy; **rebalance** without downtime is hard

---

## âœ”ï¸ Final Takeaways

> Distributed systems are **trade-offs**, not perfection.  
> Big Data â‰  Big Results without smart **data layout + scheduling + indexing**.  
> Measure, monitor, and **profile before scaling**.
