# Big Data & Distributed Systems - Interview Notes

---

## 1. Big Data Concepts

### 1. Small-file vs Big-file Problem

- In HDFS / cloud storage / Spark, **millions of tiny files** kill performance: each file requires metadata, RPCs, and task setup.  
- Small files underutilize block sizes and cause **too many tasks**, high scheduler overhead, and slow job startup.  
- Prefer **fewer, larger files** (e.g., 128–1024 MB) and compact small outputs using repartitioning or compaction jobs.  
- Common fixes: `coalesce/repartition` in Spark, periodic compaction, using formats like Parquet instead of raw CSVs.

---

### 2. Parquet Format Advantages

- **Columnar storage:** reads only needed columns → less IO, faster analytics.  
- Supports **efficient compression & encoding** per column (RLE, dictionary, delta), giving high compression ratios.  
- Works well with **schema evolution**, nested data, and integrates tightly with Spark, Hive, Presto, etc.  
- Ideal for analytics workloads, especially when combined with partitioning and pruning.

---

### 3. Spark Pitfalls Affecting Performance

- **Too many tiny partitions:** leads to task overhead; fix with `repartition` / `coalesce`.  
- **Skewed keys:** a few heavy keys cause stragglers; use salting, skew hints, or pre-aggregation.  
- **Unnecessary shuffles:** repeated `groupBy`, `join`, or wide operations; cache reuse or redesign joins.  
- **Improper caching:** caching huge DataFrames without eviction awareness → OOM.  
- **Bad file layout:** too many small files or unbalanced partitions on disk.

---

## Distributed Systems

### 1. Redis

- Redis is an **in-memory data store** often used as cache, message broker, or simple DB.  
- Supports strings, hashes, lists, sets, sorted sets, streams, and more.  
- Ideal for **low-latency reads/writes**, rate limiting, leaderboards, sessions, and distributed locks.  
- Persistence (RDB, AOF) and replication make it suitable beyond pure caching.

---

### 2. Kafka Pub-Sub (High-Level)

- Kafka is a **distributed log** for high-throughput, fault-tolerant messaging.  
- Producers write messages to **topics** split into **partitions**; consumers read in order per partition.  
- It supports both **pub-sub** (many consumers) and **queue-like** semantics (consumer groups for scaling).  
- Offsets are tracked by consumers, enabling replay and exactly-once/at-least-once patterns.

---

### 3. How Consistent Hashing Works

- In consistent hashing, keys and nodes are placed on a **ring** via a hash function.  
- A key belongs to the **first node clockwise** from its hash position.  
- When nodes join/leave, only a small fraction of keys need to move (local remapping).  
- Often uses **virtual nodes** (many positions per node) to balance load more evenly.

---

### 4. Load Balancing Algorithms (Overview)

- **Round Robin:** cycle through servers in order; simple but ignores load.  
- **Weighted Round Robin:** more traffic to stronger servers.  
- **Least Connections / Least Load:** send to server with fewest active connections or lowest load.  
- **IP Hash / Consistent Hash:** same client → same server (session affinity).  
- **Randomized / Adaptive:** combine random choice with health checks and latency metrics.

---

### 5. NoSQL Indexing (High-Level)

- NoSQL systems often use **primary key indexes** via hash or ordered structures (LSM-trees, B-trees).  
- Secondary indexes can be **local** (per shard/partition) or **global** (cover the entire cluster).  
- Common structures: inverted indexes (for text), bitmap indexes, composite keys.  
- Design is workload-driven: query patterns decide which fields get indexed to avoid full scans.

---

### 6. Consistent Hashing vs Normal Hashing

- **Normal hashing:** `hash(key) % N`; when N (number of servers) changes, almost **all keys remap**, causing huge data movement.  
- **Consistent hashing:** keys mapped on a ring; when a node changes, **only nearby keys** move.  
- Normal hashing is fine for static clusters; consistent hashing is better for **dynamic, scalable clusters**.  
- Consistent hashing plus virtual nodes smooths load and simplifies scaling out/in.


