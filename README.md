# üß† Interview Questions - ML, DL, LLMs & Systems

A curated, **high-signal collection of interview notes** for:
- Machine Learning & Deep Learning  
- LLMs, RAG & Agentic Systems  
- Big Data & Distributed Systems  
- SQL, Coding & Algorithmic Thinking  

Built as **concise, memory-friendly Q&A sheets** ‚Äî optimized for quick revision and deep-dive prep for senior ML / AI roles.

---

## üìÇ Repository Structure

Each file is a self-contained ‚Äúchapter‚Äù you can study independently.

| #  | File | Theme |
|----|------|--------|
| 1  | **[`01. ML Foundations.md`](./01.%20ML%20Foundations.md)** | Core ML concepts, regularization, bias‚Äìvariance, over/underfitting, cross-validation |
| 2  | **[`02. ML algorithms.md`](./02.%20ML%20algorithms.md)** | Popular ML models, pros/cons, assumptions, comparisons |
| 3  | **[`03. Model Validation & Debugging.md`](./03.%20Model%20Validation%20&%20Debugging.md)** | Evaluation, error analysis, debug checklists, robustness |
| 4  | **[`04. Feature Engg & Selection.md`](./04.%20Feature%20Engg%20&%20Selection.md)** | Feature engineering patterns, selection techniques, alpha/beta experiments |
| 5  | **[`05. Statistics for ML.md`](./05.%20Statistics%20for%20ML.md)** | Correlation, CLT, hypothesis testing, probability puzzles, agreement metrics |
| 6  | **[`06. Coding Problems ML Style.md`](./06.%20Coding%20Problems%20ML%20Style.md)** | Array/coding patterns with ML flavor (intersection, max product, combinations, etc.) |
| 7  | **[`07. DL Fundamentals.md`](./07.%20DL%20Fundamentals.md)** | Deep learning basics, backprop, activations, vanishing gradients, batching |
| 8  | **[`08. Neural Network Architectures.md`](./08.%20Neural%20Network%20Architectures.md)** | ANN, CNN, RNN, LSTM, autoencoders, VAEs, memory & capsule networks |
| 9  | **[`09. Transformers & Attention.md`](./09.%20Transformers%20&%20Attention.md)** | Self-attention, Q/K/V, positional encodings, Transformer pros/cons |
| 10 | **[`10. Advanced DL.md`](./10.%20Advanced%20DL.md)** | ResNets, FlashAttention, saddle points, natural gradient, label smoothing |
| 11 | **[`11. LLM Fundamentals.md`](./11.%20LLM%20Fundamentals.md)** | Transformers for LLMs, special tokens, sampling (T, top-k, top-p), seeds |
| 12 | **[`12. LLM Training Paradigms.md`](./12.%20LLM%20Training%20Paradigms.md)** | Self-supervision, RLHF, alignment, DPO, AE vs VAE vs diffusion |
| 13 | **[`13. LLM Optimization & Serving.md`](./13.%20LLM%20Optimization%20&%20Serving.md)** | VRAM estimates, quantization, LoRA/QLoRA, batching, on-device inference |
| 14 | **[`14. Diffusion Models.md`](./14.%20Diffusion%20Models.md)** | DDPM vs DDIM, forward/reverse processes, score matching, Stable Diffusion |
| 15 | **[`15. RAG & Agentic Systems.md`](./15.%20RAG%20%26%20Agentic%20Systems.md)** | RAG patterns, query routing/expansion, code/SQL RAG, ColBERT, agents |
| 16 | **[`16. Model hallucination.md`](./16.%20Model%20hallucination.md)** | Hallucination patterns, detection and mitigation strategies |
| 17 | **[`17. NLP.md`](./17.%20NLP.md)** | NLP basics, classical tasks, Transformers in NLP, BERT-style questions |
| 18 | **[`18. Big Data & Distributed Systems.md`](./18.%20Big%20Data%20%26%20Distributed%20Systems.md)** | Spark pitfalls, Parquet, small-file problem, Redis, Kafka, hashing, load balancing |
| 19 | **[`19. SQL & Databases.md`](./19.%20SQL%20%26%20Databases.md)** | SQL order of execution, joins, BETWEEN vs IN, SQL-RAG strategy |
| 20 | **[`20. Coding + Algorithms.md`](./20.%20Coding%20+%20Algorithms.md)** | Core coding patterns, distances, swaps, Bloom/XOR filters, etc. |

---

## üß© What This Repo Is (and Isn‚Äôt)

**This repo _is_:**
- A set of **short, high-yield explanations** designed to be read and revised quickly.
- Focused on **conceptual questions** that come up repeatedly in ML / DL / LLM interviews.
- Written to help you **recall under pressure** (whiteboard / live coding / system design rounds).

**This repo is _not_:**
- A full textbook or exhaustive theory reference.
- A LeetCode clone with full problem implementations.
- A replacement for hands-on coding / building real projects.

Use it as your **companion notebook**, not the only source.

---
