# Transformers & Attention - Interview Notes

---

## 1. What is Self-Attention?

Self-attention lets each token in a sequence **attend to every other token**, computing a weighted sum of their representations.  
It answers: “For this word, which other words in the sequence matter, and how much?”.  
This enables the model to capture **long-range dependencies** without recurrence.  
It’s the core operation in Transformers for both NLP and vision.

---

## 2. Why Q, K, V?

Self-attention uses three learned projections of the input: **Queries (Q)**, **Keys (K)**, and **Values (V)**.  
Q and K interact to produce attention scores (similarities), while V is what gets averaged using these scores.  
Separating them allows the model to learn **different subspaces for matching (Q/K) and content (V)**.  
This adds flexibility and expressiveness compared to a single shared representation.

---

## 3. Mean/Average in Attention

Attention output is essentially a **weighted average** of value vectors.  
Weights come from softmax over Q·K scores, so they sum to 1 across the sequence.  
If all scores are equal, attention reduces to a **simple mean** of all V vectors.  
This view helps understand attention as “learned, content-aware averaging”.

---

## 4. Drawbacks of Transformers

- **Quadratic complexity** in sequence length due to all-to-all attention.  
- Need explicit **positional encoding** (no inherent notion of order).  
- Data- and compute-hungry; hard to train from scratch without massive resources.  
- Can still hallucinate, be brittle to distribution shift, and are not inherently interpretable.

---

## 5. Transformer-Specific Normalization & Regularization

Transformers typically use **LayerNorm** (before or after attention/FFN blocks) to stabilize training.  
They rely heavily on **residual connections** plus dropout in attention probabilities and FFN layers.  
More advanced variants use **RMSNorm**, **Pre-LN** architectures, label smoothing, and stochastic depth.  
These techniques keep gradients stable in very deep stacks of attention blocks.

---

## 6. CNNs for Translation → Why They Were Replaced

CNNs with dilations and attention-like pooling were used for seq2seq tasks before Transformers.  
They struggle with capturing **global dependencies** and require many layers or large kernels to see long context.  
Transformers model long-range relationships **directly via self-attention**, regardless of distance.  
They also parallelize better across sequence positions than RNNs/CNNs in many setups.

---

## 7. Projection of Q/K/V vs Raw Vectors

Using learned linear projections for Q/K/V lets the model learn **task-specific similarity metrics**.  
If you used raw vectors, attention would rely on a fixed notion of similarity, limiting expressiveness.  
Projections can disentangle “what to match on” (Q/K space) from “what information to pass” (V space).  
Multi-head attention further allows multiple different projections in parallel.

---

## 8. Attention Masks

Attention masks control **which positions a token is allowed to attend to**.  
- Padding mask: prevents attending to padding tokens.  
- Causal mask: blocks attention to future tokens in autoregressive models.  
Masks are applied by setting disallowed positions’ scores to a very negative value before softmax.  
They enforce structural constraints like directionality and sequence validity.

---

## 9. Absolute vs Relative Positional Encoding

**Absolute positional encoding:** assign each position a unique vector (e.g., sinusoidal or learned embedding) added to token embeddings.  
**Relative positional encoding:** encode the relation between positions (e.g., distance i–j) directly in the attention scores.  
Relative encodings generalize better to longer sequences and model patterns like “previous token” or “next token” more naturally.  
Absolute encodings are simpler but more rigid for extrapolating beyond trained lengths.

---

## 10. Why Sinusoidal Encoding?

Sinusoidal encodings give each position a deterministic embedding using sine/cosine at different frequencies.  
They allow the model to infer **relative positions** from linear combinations of these signals.  
No learned parameters are needed, and they can **extrapolate** to longer sequences than seen in training.  
They were the original choice in “Attention Is All You Need”.

---

## 11. RoPE (Rotary Positional Embeddings) & Long-Context Extension

RoPE encodes position by **rotating** Q and K vectors in a complex or 2D subspace based on their position index.  
This makes attention scores depend on **relative positions** in a smooth, rotation-based way.  
RoPE generalizes nicely when extending context length with careful scaling or interpolation.  
It is widely used in modern LLMs to support long context windows.

---

## 12. All-to-All Interactions vs Attention

A naive all-to-all interaction (e.g., full fully connected between tokens) would be expensive and unstructured.  
Self-attention is a **structured all-to-all interaction** where influence is controlled by learned similarity scores.  
It lets any token attend to any other, but with **content-dependent weights** and parameter sharing.  
This is more efficient and flexible than fixed or dense pairwise interactions.

```

