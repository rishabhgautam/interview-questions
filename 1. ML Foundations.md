
# **ML Foundations — Interview Notes**

---

## **1. L1 vs L2 Regularization**

L1 adds an absolute-value penalty that drives some weights to zero, creating sparse models and performing feature selection.

L2 adds a squared-weight penalty that shrinks weights smoothly and distributes influence across all features.

L1 is better for eliminating irrelevant features, while L2 is better for stability and handling multicollinearity.

L2 is easier to optimize because the penalty is smooth and differentiable.

---

## **2. Bias, Variance & the Trade-off**

Bias is error from overly simple assumptions; variance is error from excessive sensitivity to training data.

High bias causes underfitting, while high variance causes overfitting.

The goal is to find a balance where neither dominates.

Total error = Bias² + Variance + Irreducible noise.

---

## **3. Overfitting & How to Prevent It**

Overfitting happens when the model memorizes training data instead of learning general patterns.

High training accuracy but low test accuracy is a strong sign.

Prevent it using regularization, dropout, early stopping, data augmentation, and simpler architectures.

Good validation strategies and more training data also help.

---

## **4. Underfitting**

Underfitting occurs when a model is too simple to capture patterns in the data.

Both training and test scores are low.

Fix it by increasing model complexity, training longer, adding more features, or reducing regularization.

---

## **5. Cross-validation**

Cross-validation evaluates model generalization across multiple train/validation splits.
It reduces dependence on a single split and provides a more stable performance estimate.

It is essential for hyperparameter tuning and detecting overfitting. K-fold CV is the most widely used variant.

---

## **6. Gradient Descent + Local vs Global Minima**

Gradient Descent updates parameters in the direction that reduces loss using computed gradients.

Neural networks have non-convex loss surfaces with many local minima and saddle points.

A global minimum is the absolute lowest loss, but reaching it is not guaranteed.

Modern optimizers like Adam help escape poor minima and saddle points effectively.

---

## **7. Batch vs Stochastic vs Mini-batch Gradient Descent**

Batch GD updates using the entire dataset—stable but slow and memory-heavy.

Stochastic GD updates using one sample—fast but noisy and unstable.

Mini-batch GD (32–512 samples) balances speed, noise, and GPU utilization.

It is the default method in deep learning training.

---

## **8. Batch Normalization**

BatchNorm normalizes layer activations using mini-batch statistics and then applies learnable scale/shift.

It stabilizes training, allows higher learning rates, and smooths the loss landscape.

Acts as a mild regularizer by injecting noise from batch statistics.

Widely used in CNNs and feedforward networks.

---

## **9. Active Learning**

Active Learning selects the most informative unlabeled samples to label, reducing annotation effort.

Strategies include uncertainty sampling, query-by-committee, and diversity-based sampling.

Useful when labeled data is expensive but unlabeled data is abundant.

Different from online learning, which processes samples sequentially instead of selectively.

---

## **10. Weak Supervision**

Weak supervision relies on noisy or heuristic labels instead of manually annotated ground truth.

Examples include label functions, distant supervision, rules, and pattern matching.

Effective when large labeled datasets are impractical.

Requires aggregation or denoising to generate reliable training labels.

---

## **11. Selection Bias**

Selection bias occurs when the training sample does not represent the true population.
This creates biased models and poor generalization.

Prevent it through random sampling, stratified sampling, and careful data collection.
Monitor data drift to avoid bias developing over time.

---

## **12. Handling Imbalanced Data**

Use methods like oversampling (SMOTE), undersampling, and class-weighted loss functions.

Balanced batches and synthetic sample generation help improve representation of minority classes.

Focus on metrics like F1-score, AUC, and Precision-Recall curves.

Tree ensembles and gradient boosting models often handle imbalance well.

---

## **13. Fixing Distribution Shift**

Distribution shift occurs when training and test data come from different distributions.

Fix it using domain adaptation, reweighting samples, robust feature engineering, or retraining with fresh data.

Monitoring drift helps detect shift early.

Production ML requires continuous feedback and retraining loops.

---

## **14. Dummy Variable Trap**

The dummy variable trap arises when all dummy columns of a categorical variable are included, causing multicollinearity.

Drop one dummy column to set a baseline category.

Most ML libraries handle this automatically (drop_first).

This keeps coefficients identifiable and stable.

---

## **15. Supervised vs Unsupervised Learning**

Supervised learning uses labeled data to learn mappings (classification, regression).

Unsupervised learning discovers patterns in unlabeled data (clustering, PCA).

Supervised focuses on prediction; unsupervised focuses on structure understanding.

Semi-supervised and self-supervised combine principles from both.

---

## **16. Decision Boundary vs Decision Surface**

A decision boundary is the separator between classes in 2D feature space.
In higher dimensions, this generalizes to a decision surface.

Linear models produce straight boundaries; non-linear models produce curved ones.
The surface defines regions where the model predicts each class.

---

## **17. Generative vs Discriminative Models**

Generative models learn **P(x, y)** or **P(x|y)** and can generate new samples (Naive Bayes, GANs).

Discriminative models learn **P(y|x)** and directly classify inputs (Logistic Regression, SVM).

Generative models capture full data distributions; discriminative models focus on boundaries.
Discriminative models usually perform better when labeled data is abundant.

---

## **18. Why MSE is Not Ideal for Classification**

MSE penalizes errors quadratically and produces poor gradients for probabilistic outputs.

It misaligns with the log-likelihood of classification tasks and slows convergence.

Cross-entropy loss gives sharper gradients and aligns with the underlying distribution (Bernoulli/Categorical).

Thus CE is the standard loss for classification.

---

## **19. When Decision Trees Are Not Useful**

Trees perform poorly on high-dimensional sparse data (e.g., text, embeddings).

They struggle with smooth or linear decision boundaries where linear models excel.

They overfit easily without pruning or ensemble methods.

Neural networks or linear models are better for continuous and highly structured features.
