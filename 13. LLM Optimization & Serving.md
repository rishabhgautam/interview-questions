# LLM Optimization & Serving - Interview Notes

---

## 1. Memory Needed to Serve an LLM
- **Components:**  
  - **Model weights:** parameters stored in FP16, FP32, or quantized formats  
  - **KV cache:** stores key/value pairs for each layer during autoregressive decoding (`layers * seq_len * hidden_dim`)  
  - **Activations:** intermediate outputs needed for computation  
  - **Runtime overhead:** tokenizer buffers, batching structures, GPU driver/runtime overhead
- **Rule of Thumb:**  
  \[
  \text{Memory} \approx \text{params} \times \text{bytes per param} + \text{KV cache} + \text{activations}
  \]
- **Optimization tips:**  
  - Quantization reduces weight storage  
  - Weight-sharing reduces redundancy  
  - Careful batching can reduce peak memory usage  

---

## 2. VRAM Requirements for a 7B Model (Example)
| Precision | Weight Memory | Notes |
|-----------|---------------|-------|
| FP16      | ~14 GB        | Just weights, additional memory needed for KV cache & activations |
| 8-bit     | ~7 GB         | Slightly faster, less memory; some quality loss |
| 4-bit     | ~3.5–4 GB     | With LoRA or QLoRA, can fine-tune or serve on 8–12 GB GPU for moderate sequence lengths |

- **KV cache & activations:** Add a few more GB depending on sequence length, batch size, and layers.

---

## 3. KV Cache (Key-Value Cache)
- **Purpose:** Speeds up autoregressive generation by **reusing past attention computations**.  
- **Mechanism:**  
  - During generation, each token requires attention to all previous tokens.  
  - Instead of recomputing `Q*K^T` for all past tokens every step, the model stores `K` and `V` for each layer in the KV cache.  
  - When generating the next token, only the new token’s `Q` interacts with cached `K`/`V`.  
- **Memory footprint:**  
  \[
  \text{KV cache memory} \approx \text{layers} \times \text{seq\_len} \times \text{hidden\_dim} \times 2 \times \text{bytes per param}
  \]  
  - Grows linearly with sequence length.  
  - Can dominate memory usage for long contexts.  
- **Optimizations:**  
  - **Offloading:** Move some KV cache to CPU/NPU for very long sequences  
  - **Chunking or windowed attention:** Keep only recent tokens in cache for ultra-long contexts  
  - **Compression:** Low-bit storage or quantized KV cache

---

## 4. Flash Attention
- **Purpose:** Efficient attention computation without materializing the full QK^T matrix.  
- **How it works:**  
  - Tiling attention computation in GPU memory  
  - Fusing operations: QK^T → softmax → dropout → V multiply  
- **Benefits:**  
  - Reduces memory I/O overhead  
  - Speeds up training and inference for long sequences  
  - Exact attention (not approximate)  

---

## 5. Continuous & Dynamic Batching
- **Continuous batching:**  
  - Keep a **global batch** updated with incoming token generation requests  
  - GPU is maximally utilized at all times  
- **Dynamic batching:**  
  - Collect requests over a short time window  
  - Form a batch on-the-fly respecting size/shape constraints  
- **Trade-offs:**  
  - Slight latency increase per request  
  - Much higher throughput and GPU utilization  

---

## 6. LoRA & QLoRA
- **LoRA (Low-Rank Adaptation):**  
  - Freeze original weight `W`  
  - Learn low-rank matrices `A` and `B` → `W_eff = W + A*B`  
  - Reduces trainable parameters drastically, enabling large model fine-tuning on smaller GPUs
- **QLoRA:**  
  - Combine **4-bit quantized base model** with **LoRA adapters**  
  - Fine-tune efficiently in higher precision only for adapters  
  - Enables training/fine-tuning of very large models on a single consumer GPU

---

## 7. On-Device Inference Strategies
- **Quantization:** Reduce precision (8-bit, 4-bit) to save memory and speed up computation  
- **Distillation:** Train smaller “student” model to mimic a large “teacher” LLM  
- **Pruning / Structured sparsity:** Remove unimportant weights, enforce hardware-friendly sparsity  
- **KV-cache & Offloading:**  
  - Cache key/value states to avoid recomputation  
  - Offload rarely used weights to CPU or NPU  
- **Goal:** Reduce memory footprint, compute, and latency while maintaining acceptable quality  

---

## 8. Practical Considerations
- Optimize **batch size and sequence length** to fit GPU memory  
- Use **mixed precision (FP16/BF16)** with careful loss scaling  
- Consider **adapter tuning** (LoRA, IA3) for multi-task or domain adaptation  
- Monitor **throughput vs latency trade-offs** when serving in production  
