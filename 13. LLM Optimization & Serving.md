# LLM Optimization & Serving - Interview Notes

---

## 1. Memory Needed to Serve an LLM

Serving memory ≈ **model weights + KV cache + activations + overhead**.  
Rough rule: `params * bytes_per_param` for weights (e.g., 16-bit ≈ 2 bytes/param), plus extra for sequence length and batch (KV cache grows with `layers * seq_len * hidden_dim`).  
Optimizers are not needed at inference, but quantization and weight-sharing can dramatically reduce memory.  
Real deployments also budget for tokenizer, runtime, and batching buffers.

---

## 2. VRAM for a 7B Model (Rule of Thumb)

- FP16: `7B * 2 bytes ≈ 14 GB` just for weights.  
- 8-bit quantization: ~7 GB; 4-bit quantization: ~3.5–4 GB.  
You still need extra headroom for KV cache and activations (often a few more GB depending on sequence length and batch size).  
In practice, 7B 4-bit can run on a ~8–12 GB GPU for moderate context and batch sizes.

---

## 3. Flash Attention

FlashAttention is a **memory-efficient attention kernel** that avoids materializing the full attention matrix.  
It computes attention in tiled blocks in GPU memory, fusing operations (QK^T, softmax, dropout, V multiply).  
This reduces memory usage and I/O, often speeding up training and inference, especially for long sequences.  
It is exact attention, not an approximation; it’s a better implementation, not a different algorithm.

---

## 4. Continuous & Dynamic Batching

**Continuous batching:** The server keeps a **global batch** and keeps adding/removing requests as tokens are generated, so the GPU is always busy.  
**Dynamic batching:** Incoming requests over a short time window are grouped into a batch on the fly based on size/shape constraints.  
Both techniques greatly improve throughput and GPU utilization for LLM serving.  
Trade-off: slightly higher latency per individual request vs much higher overall throughput.

---

## 5. QLoRA, LoRA – How LoRA Works

**LoRA (Low-Rank Adaptation):**  
- Instead of updating full weight matrix `W`, you keep `W` frozen and learn low-rank matrices `A` and `B` such that: `W_eff = W + A * B`.  
- Rank (r) is small, so the number of trainable params is tiny compared to full `W`.  
This enables efficient fine-tuning of large models with much less memory.

**QLoRA:** Combines **4-bit quantization** of base weights with LoRA adapters on top.  
You pre-quantize the model and only train the small LoRA layers in higher precision, enabling fine-tuning of large LLMs on a single GPU.

---

## 6. On-Device Inference Strategies

- **Quantization:** Use 8-bit or 4-bit weights (and sometimes activations) to fit the model on limited VRAM / mobile devices.  
- **Distillation:** Train a smaller “student” model to mimic a larger “teacher” LLM.  
- **Pruning & structured sparsity:** Remove less important weights/neurons or enforce sparse patterns that hardware can exploit.  
- **KV-cache & offloading:** Cache past key/values and optionally offload some weights/compute to CPU or NPU.  
All aim to reduce memory, compute, and latency while keeping quality acceptable on edge devices.


