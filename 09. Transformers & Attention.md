# Transformers & Attention - Interview Notes

---

## 1. What is Self-Attention?

**Self-attention** allows each token in a sequence to **attend to every other token**, computing a weighted sum of their representations.  

- **Purpose:** Capture long-range dependencies without sequential recurrence.  
- **Computation:** For each token, compute attention scores with every other token → softmax → weighted sum of values.  
- **Formula:**  
```

Attention(Q,K,V) = softmax(Q K^T / sqrt(d_k)) V

```
- `Q, K, V` = Queries, Keys, Values matrices  
- `d_k` = dimension of the key vectors (scaling factor)  

- Benefits:  
- Parallelizable across sequence positions  
- Models dependencies regardless of distance  
- Basis of Transformers in NLP, vision, speech  

---

## 2. Why Q, K, V?

- Q (Query): what I’m looking for  
- K (Key): what each token contains  
- V (Value): information to propagate  

- **Mechanism:** similarity between Q and K → attention weights → weighted sum of V  
- **Why separate?**  
- Flexibility: different subspaces for matching vs content propagation  
- Multi-head attention: multiple Q/K/V projections capture different relationships simultaneously  

---

## 3. Mean/Average in Attention

- Self-attention output = weighted average of values:  
```

out_i = sum_j softmax(score(Q_i, K_j)) * V_j

```
- If attention scores are equal → simple mean of all V vectors.  
- Intuition: model learns **dynamic, context-aware averaging** rather than fixed pooling.  

---

## 4. Drawbacks of Transformers

- **Quadratic complexity**: O(n²) attention for sequence length n  
- Need **positional encoding** → no inherent sequence ordering  
- **Resource intensive** → training from scratch requires large GPUs/TPUs  
- Can **hallucinate** or be brittle under distribution shift  
- Limited interpretability; difficult to understand exact reasoning  

---

## 5. Transformer-Specific Normalization & Regularization

- **LayerNorm:** stabilizes training → applied pre/post-attention & feedforward  
- **Residual connections:** allow gradients to flow through deep layers  
- **Dropout:** applied to attention probabilities and feedforward layers to reduce overfitting  
- Variants & tricks:  
- RMSNorm, Pre-LN Transformers  
- Label smoothing → prevents overconfident predictions  
- Stochastic depth → randomly skip layers during training for regularization  

---

## 6. CNNs for Translation → Why Replaced

- Pre-Transformer seq2seq models: CNN + attention pooling  
- **Limitations:**  
- Require large receptive fields for long-range dependencies  
- Deep stacking → slow, high parameter count  
- Transformers:  
- Capture **global context** directly via self-attention  
- Highly parallelizable → faster training/inference  
- Simplifies architecture without recurrent or convolutional inductive bias  

---

## 7. Projection of Q/K/V vs Raw Vectors

- Learned linear projections:  
```

Q = X W_Q,  K = X W_K,  V = X W_V

```
- Benefits:  
- Learn **task-specific similarity metrics**  
- Separate representation space for matching vs information content  
- Multi-head attention: multiple projections in parallel → capture diverse relationships  
- Without projection → attention relies on raw embeddings → less expressive  

---

## 8. Attention Masks

- Control **allowed attention positions**:  
- **Padding mask:** ignore padding tokens in batch sequences  
- **Causal mask:** prevent attending to future tokens in autoregressive tasks  
- Implementation: set masked positions’ logits to **-inf** before softmax  
- Enforces structural constraints → ensures model adheres to valid dependencies  

---

## 9. Absolute vs Relative Positional Encoding

- **Absolute:** assign fixed positional vector → added to token embeddings  
- Sinusoidal (deterministic) or learned embeddings  
- Simpler but rigid for unseen lengths  
- **Relative:** encode token-token distances in attention scores  
- Generalizes better to longer sequences  
- Captures patterns like “next token” / “previous token” relationships  

---

## 10. Why Sinusoidal Encoding?

- Deterministic formula:  
```

PE(pos,2i)   = sin(pos / 10000^(2i/d_model))
PE(pos,2i+1) = cos(pos / 10000^(2i/d_model))

```
- Advantages:  
- Model can infer **relative distances** from linear combinations  
- No additional parameters → lightweight  
- Extrapolates beyond training sequence lengths  

---

## 11. RoPE (Rotary Positional Embeddings) & Long-Context

- Encode position by **rotating Q/K vectors** in complex or 2D space  
- Attention score depends on relative positions via rotation angle  
- Advantages: smooth generalization to longer sequences, supports **extended context LLMs**  
- Used in modern models like **LLaMA, GPT-4, Mistral**  

---

## 12. All-to-All Interactions vs Attention

- Naive all-to-all: dense fully connected between tokens → huge parameter count, unstructured  
- Self-attention: structured all-to-all with **content-dependent weights**  
- Efficient: O(n²) complexity but parameter sharing reduces memory footprint  
- Flexible: any token can attend to any other, weighted dynamically  

---

## 13. Multi-Head Attention

- Split Q/K/V into `h` heads → parallel attention computations  
- Each head attends to different subspace → captures **diverse relationships**  
- Concatenate outputs of all heads → linear projection → final output  
- Formula:  
```

MultiHead(Q,K,V) = Concat(head_1,...,head_h) W_O
head_i = Attention(Q_i, K_i, V_i)

```

---

## 14. Transformer Encoder vs Decoder

| Component | Encoder | Decoder |
|-----------|--------|---------|
| Attention | Self-attention | Masked self-attention + cross-attention to encoder |
| Inputs | Token embeddings + positional encoding | Previous outputs + positional encoding |
| Output | Contextualized embeddings | Predictions (autoregressive) |

---

## 15. Key Transformer Variants / Extensions

- **BERT:** encoder-only, bidirectional, MLM pretraining  
- **GPT:** decoder-only, autoregressive, next-token prediction  
- **T5:** encoder-decoder, text-to-text paradigm  
- **ViT:** vision transformer → patches as tokens  
- **Longformer, BigBird:** efficient sparse attention for long sequences  
- **Perceiver:** cross-attention → modality-agnostic processing  

---

## 16. Practical Notes / Interview Tips

- Self-attention = **learned, weighted averaging** over sequence  
- Multi-head → multiple independent attention patterns  
- RoPE/relative PE → preferred for **long-context LLMs**  
- Attention masks enforce **directionality & structure**  
- Transformers trade off **compute & memory** for expressiveness → key interview talking point  
- Always link **mechanism → intuition → practical application**
