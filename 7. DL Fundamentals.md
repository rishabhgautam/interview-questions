# Deep Learning (DL) - Fundamentals

---

## 1. What is Deep Learning?

Deep Learning is a subset of Machine Learning that uses **multi-layer neural networks** to learn hierarchical representations from data.  
Instead of manual feature engineering, deep models learn features directly from raw inputs (images, text, audio, etc.).  
It scales very well with large datasets and compute (GPUs/TPUs).  
Most modern NLP, vision, and speech systems are powered by DL.

---

## 2. Difference vs Traditional ML

Traditional ML relies heavily on **hand-crafted features** plus relatively simple models (logistic regression, trees, SVMs).  
Deep Learning uses **deep neural networks** to learn features and decision logic jointly, often from raw data.  
DL shines on unstructured data (images, audio, text), while classical ML often dominates on small tabular datasets.  
DL typically requires more data, compute, and careful training.

---

## 3. Backpropagation

Backpropagation is the algorithm used to **compute gradients** of the loss with respect to all network parameters.  
It applies the **chain rule** layer by layer in reverse, propagating error signals from output to input.  
Once gradients are known, optimizers like SGD or Adam update the weights.  
Backprop is the engine that makes training deep networks practical.

---

## 4. Why Mini-Batch Training?

Mini-batches let you process multiple samples at once, leveraging **GPU parallelism** efficiently.  
They provide a **noisy but good** estimate of the full gradient, which often improves generalization.  
Full-batch is slow and memory-heavy; single-sample SGD is too noisy.  
Mini-batches strike a balance between speed, stability, and noise.

---

## 5. Representation Learning

Representation learning is about learning **useful feature representations** directly from data.  
Deep networks transform raw inputs through multiple layers into abstract features (edges → shapes → objects; words → phrases → meaning).  
Good representations make downstream tasks (classification, retrieval, generation) easier and more robust.  
Pretrained models (BERT, ResNet, etc.) are powerful representation learners reused across tasks.

---

## 6. Dropout

Dropout randomly “drops” (sets to zero) a fraction of activations during training.  
This prevents neurons from co-adapting too strongly and acts as a form of **regularization**.  
At inference time, all neurons are used, but weights/activations are scaled appropriately.  
It helps reduce overfitting, especially in fully connected layers.

---

## 7. Weight Initialization

Weight initialization sets starting values for network parameters before training begins.  
Good initialization keeps activations and gradients in a **reasonable range** so signals neither vanish nor blow up.  
He/Kaiming init (for ReLU) and Xavier/Glorot init (for tanh/sigmoid) are common schemes.  
Bad initialization can stall learning or make optimization unstable.

---

## 8. Data Augmentation

Data augmentation creates new training samples by **transforming existing ones** without changing the label.  
Examples: flips/crops/rotations for images, noise/time-shift for audio, paraphrasing or masking for text.  
It increases effective dataset size and improves robustness and generalization.  
Crucial when labeled data is limited.

---

## 9. Overfitting in DL → Mitigations

Deep networks can easily **memorize** training data, especially with many parameters.  
Mitigations include dropout, L2 regularization (weight decay), data augmentation, and early stopping.  
You can also reduce model size or use stronger regularizers like label smoothing or mixup.  
Good validation splits and monitoring are key.

---

## 10. Vanishing & Exploding Gradients

Vanishing gradients: gradients become extremely small as they propagate back, slowing or stopping learning in early layers.  
Exploding gradients: gradients become extremely large, causing unstable updates and divergence.  
Mitigations: better initialization, ReLU-family activations, gradient clipping, residual connections, normalization (BatchNorm/LayerNorm).  
These issues were a major driver behind modern architectures like ResNets and LSTMs.

---

## 11. Activation Functions (ReLU, Sigmoid, Tanh, etc.)

Activation functions introduce **non-linearity**, allowing networks to model complex relationships.  
- **ReLU:** `max(0, x)`; si
