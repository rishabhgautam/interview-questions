# LLM Fundamentals - Interview Notes

---

## 1. What is a Transformer?
- Neural network architecture based on **self-attention** rather than recurrence or convolutions.  
- Each token attends to **all other tokens** in a sequence to capture context.  
- Key components:
  - **Multi-Head Self-Attention:** enables the model to attend to different representation subspaces simultaneously.
  - **Feedforward Layers (MLPs):** position-wise transformations for non-linear feature extraction.
  - **Residual Connections + LayerNorm:** stabilizes gradients and improves training for deep stacks.
  - **Positional Encodings:** provide token order information (absolute, relative, or RoPE).  
- Transformers are the backbone of modern LLMs like **GPT, BERT, LLaMA, and PaLM**.  
- Advantages:
  - Efficient parallelization across tokens
  - Captures long-range dependencies without sequential recurrence
  - Scales well with model and dataset size  

---

## 2. Why Initialize Weights Randomly?
- **Break symmetry:** prevents neurons from learning identical features.  
- **Stabilize gradients:** Xavier/Glorot or He initialization keeps activations in reasonable ranges.  
- **Speeds up convergence:** avoids extremely small/large initial outputs that slow learning.  
- Without proper initialization:
  - Gradients can vanish or explode
  - Training becomes unstable
  - Network fails to learn diverse features  

---

## 3. Weight Tying
- **Definition:** Reuse a weight matrix across multiple parts of the network.  
- **Common use in LLMs:**  
  - Share input token embeddings with output softmax projection.  
- **Benefits:**
  - Reduces parameter count → memory efficient
  - Improves generalization by enforcing consistency between encoding and decoding spaces
  - Speeds up training slightly by reducing redundant learning  

---

## 4. Special Tokens
- Tokens with **non-linguistic roles** to guide model understanding:  
  - **BOS / EOS:** Beginning/End of sequence  
  - **PAD:** Fill shorter sequences to uniform length  
  - **SEP:** Separate sentences or segments  
  - **CLS:** Classification token for sentence-level tasks  
  - **Role tokens:** SYS / USER / ASSISTANT for instruction-following or dialogue models  
- **Usage:** define structure, boundaries, and roles, crucial in dialogue and generative tasks  

---

## 5. Attention Masks
- **Purpose:** control which tokens a token can attend to.  
- Types:
  - **Padding Mask:** ignores padded tokens to prevent them influencing attention
  - **Causal Mask:** prevents a token from attending to future tokens (autoregressive models)
- **Implementation:** disallowed positions get very negative scores before softmax  
- **Impact:** ensures correct sequence modeling, prevents leakage of future information  

---

## 6. Decoding: Temperature, Top-k, Top-p, Sampling
- **Temperature (T):** scales logits before softmax
  - Low T → deterministic, sharp distributions (argmax-like)
  - High T → diverse, probabilistic outputs
- **Top-k Sampling:** pick from k highest-probability tokens
- **Top-p (Nucleus) Sampling:** pick smallest token set with cumulative probability ≥ p
- **Greedy / Argmax Decoding:** deterministic, always chooses the most probable token
- **Beam Search:** keeps multiple hypotheses to maximize sequence probability  
- **Trade-off:** creativity vs reliability; higher randomness → more novel but potentially inconsistent outputs  

---

## 7. Determinism in LLM Outputs
- **Stochastic Generation:** outputs vary with temperature, top-k/top-p sampling
- **Deterministic Generation:**  
  - Argmax decoding (T=0)
  - Fixed random seed
- **Key insight:** Same model + same prompt + same decoding parameters + same seed → reproducible outputs
- Useful in evaluation, testing, and deployment  

---

## 8. Random Seed & Reproducibility
- **Random seed:** initializes pseudorandom number generator used in sampling
- Fixing seed → reproducible outputs  
- Changing seed → different plausible outputs  
- Best practice for experiments: document seed along with model, prompt, and decoding strategy  

---

## 9. Positional Encoding
- Transformers lack inherent order awareness → **positional encoding** is essential
- **Absolute encoding:** fixed vector per position (sinusoidal or learned)  
- **Relative encoding:** encodes position difference between tokens  
- **RoPE (Rotary):** rotates Q/K vectors to encode positions → smooth extrapolation for long sequences  
- **Impact:** allows model to reason about sequence order, distance, and context  

---

## 10. Tokenization & Embeddings
- **Subword tokenization:** Byte Pair Encoding (BPE), SentencePiece, WordPiece  
- Breaks text into manageable units → reduces OOV issues  
- **Embedding layer:** converts tokens into dense vectors → input to Transformer blocks  
- **Embedding dimension** usually matches model hidden size  
- Weight tying often connects embeddings to output logits  

---

## 11. Model Scaling Principles
- **Depth:** more layers → richer hierarchical features  
- **Width:** larger hidden dimension → better expressiveness  
- **Context length:** longer sequences → better for tasks requiring memory  
- **Parameter efficiency tricks:** weight tying, attention sparsity, flash attention  

---

## 12. Practical Tips for Interviews
- Be able to explain **self-attention & QKV intuition**  
- Discuss **stochastic vs deterministic decoding** and **seed control**  
- Explain **special tokens, padding, causal masks** in context of tasks  
- Highlight **weight initialization and weight tying** benefits for LLM training  
- Show understanding of **positional encoding** (absolute, relative, RoPE)  
- Discuss **decoding strategies**: temperature, top-k, top-p, beam search  
