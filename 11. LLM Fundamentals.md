# LLM Fundamentals - Interview Notes

---

## 1. What is a Transformer?

A Transformer is a neural network architecture based on **self-attention** instead of recurrence or convolutions.  
Each token attends to all other tokens in a sequence to aggregate context, enabling long-range dependency modeling.  
It uses stacked blocks of **multi-head attention + feedforward layers + residual connections + normalization**.  
Transformers are the backbone of modern LLMs (GPT, BERT, etc.).

---

## 2. Why Initialize Weights Randomly?

Random initialization breaks **symmetry** so neurons don’t learn identical features.  
If all weights started equal, gradient updates would stay equal and the network would behave like a single neuron.  
Carefully chosen random schemes (Xavier, He) also keep activations and gradients in a reasonable range.  
Good initialization speeds up convergence and stabilizes training.

---

## 3. Weight Tying

Weight tying reuses the **same weight matrix** for multiple parts of the model (e.g., input embeddings and output softmax layer).  
This reduces the number of parameters and often improves generalization by enforcing consistency between encoding and decoding spaces.  
Common in language models: the token embedding matrix is shared with the output projection.  
It also saves memory, which matters a lot at LLM scale.

---

## 4. Special Tokens

Special tokens are reserved tokens with **non-linguistic roles**, such as:  
- BOS / EOS: beginning/end of sequence  
- PAD: padding  
- SEP: separator (e.g., between sentences)  
- CLS / SYS / USER / ASSISTANT: for classification or role encoding  
They help structure the input and guide the model about boundaries, roles, and formatting.

---

## 5. Attention Masks

Attention masks specify **which positions a token is allowed to attend to**.  
Padding masks prevent attention to padded positions; causal masks prevent looking at future tokens in autoregressive models.  
Technically, disallowed positions’ scores are set to a very negative value before softmax.  
Masks enforce sequence rules and are crucial for correctness.

---

## 6. Temperature, Top-k, Top-p, Sampling

- **Temperature:** scales logits; low T → deterministic, sharp; high T → diverse, noisy.  
- **Top-k sampling:** sample only from the k most probable tokens at each step.  
- **Top-p (nucleus) sampling:** sample from the smallest set of tokens whose cumulative probability ≥ p.  
These controls trade off **creativity vs reliability** in LLM generations.

---

## 7. Does an LLM Produce Different Answers on the Same Input?

Yes, if **sampling** is used (non-zero temperature, top-k/top-p), generation is inherently stochastic.  
With the same settings and seed, outputs can be made repeatable; without a fixed seed, you’ll see variation.  
If you set temperature to 0 (argmax decoding), the model becomes deterministic given the exact same context.  
So behavior depends on decoding strategy and seed control.

---

## 8. Seed & Randomness

A **random seed** initializes the pseudorandom number generator used in sampling.  
Fixing the seed makes the sequence of random choices (e.g., sampled tokens) **reproducible** for the same input and settings.  
Changing the seed changes which tokens are sampled when multiple are plausible.  
In practice: same prompt + same model + same parameters + same seed → same output.


